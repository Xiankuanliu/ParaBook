\chapter{R 并行处理入门}
\label{chap:r}

\section{为什么要在本书中用 R 语言？}

在本书的其它章节里，C/C++依然是我们的主要语言，但我们也提供
很多 R 语言的示例。为什么要用 R 呢？

\begin{itemize}

\item R 是最广泛使用的用于统计分析和数据处理的编程语言。在现今
这个大数据时代，人民已经开发了相当数量的用于并行计算的 R 扩展包。
特别地，{\bf parallel} 扩展包现在已经是 R 基础包的一部分。

\item R 语言的广泛使用，从 Google 设置了其内部的 R 语言规范一事
就可见一斑\footnote{个人角度来讲，我并不喜欢这些代码规范，
我更喜欢我自己的。但从 Google 设置
自己的 R 语言规范可以看出他们对 R 的重视程度。}。现在 Oracle 也
把 R 包含了自己的大数据分析方案中。

\item 对于展示各种各样的并行算法，R 非常方便。这点的主要原因
在于 R 内置了向量、矩阵和复数类型。

\end{itemize}

Python 也有很多并行库，比如 {\bf multiprocessing}。关于 Python 的
并行话题，我们会在第\ref{chap:pythr}章里讨论。

{\bf 本章的示例会保持尽量简单。}但 R 中的并行计算也可以应用到非常庞大
而复杂的问题上。在附录\ref{chap:rquickstart}中，有一个5分钟的 R 快速入门。
阅读时请牢记 R 中 list 结构。

{\bf R 中进行并行计算的关键就是—— list 结构的操作。}许多 R 的并行计算
扩展包都非常依赖于 R 中的 list 结构。输入输出的参数和返回值经常
都采用 list 的形式。读者可能有
兴趣参考一下附录\ref{chap:rquickstart}中的相关内容。

\section{R 和易并行问题（Embarrassing Parallel Problems）}

需要注意的是，R 的并行扩展包一般只能处理易并行问题。正如在
\ref{embpar}节中定义的，这些问题不仅容易并行化，而且信息传递
的需求很少\footnote{后面的要求把很多迭代算法排除在外了，
尽管它们很容易并行化。}。如我们所知，一般只有易并行问题会有
很好的表现，但在 R 中情况尤其如此，原因如下。

R 语言的函数式编程的本质意味着，任何对一个向量或矩阵的元素的写入操作，
比如
\begin{lstlisting}
x[3] <- 8
\end{lstlisting}
都会重写整个向量或矩阵\footnote{R 中的元素赋值是一个函数调用，
上面这个例子的参数分别为 {\bf x}、3和8。}。虽然有些例外（随着 R 版本
更新，例外可能越来越多），但一般来说我们必须承认 R 中并行的向量
和矩阵代码代价很高\footnote{R 中新的引用类（Reference class）可能会对此有所改变。}。

对于不易并行的问题，大家应该考虑用 R 调用并行的 C 代码，这点会在\ref{cfromr}
节中讨论。

\section{一些 R 的并行扩展包}

这里我们列举了一些 R 的并行扩展包：

\begin{itemize}

\item Message-passing 或 scatter/gather (\ref{scattergather}节)：
{\bf Rmpi}、{\bf snow}、{\bf foreach}、{\bf rmr}、{\bf Rhipe}、{\bf multicore}\footnote{{\bf multicore} 扩展包运行于多核
内存共享的平台之上，但在读写过程中并不共享数据。}、{\bf rzmq}

\item 内存共享：{\bf Rdsm}、{\bf bigmemory}

\item GPU：{\bf gputools}、{\bf rgpu}

\end{itemize}

大家可以从
\url{http://cran.r-project.org/web/views/HighPerformanceComputing.html}找到更加详尽
的列表。

从2.14版本开始，R 默认包括了由 {\bf snow} 和 {\bf multicore} 构成
的 {\bf parallel} 扩展包。（早期版本可能需要分别下载。）
正是因为如此，二者都在范围之内。另外，我们也会讨论
{\bf Rdsm/bigmemory} 和 {\bf gputools}。

\section{安装和载入这些扩展包}

{\bf 安装：}

需要注意的是，如果你使用的是2.14版或更高版本的 R，你已经安装
了 {\bf snow} 和 {\bf multicore}。一般来说，除了 {\bf rgpu}，
其它所有扩展包都可以从 R 官方的代码仓库 CRAN（\url{http://cran.r-project.org}）下载。
这里以 {\bf snow} 为例：

加入你想把它安装在 {\bf /a/b/c/} 目录下。最简单的方法就是
使用 R 的函数：

\begin{lstlisting}
> install.packages("snow","/a/b/c/")
\end{lstlisting}

这会将 {\bf snow} 安装在 {\bf /a/b/c/{\bf snow}} 目录下。

之后你需要将目录{\bf /a/b/c}（不是{\bf
/a/b/c/snow}）加到你的 R 搜索路径中。我推荐大家在
自己 home 目录下的{\bf .Rprofile}文件（这是 R 的启动设置文件）中添加这样一行。
\begin{lstlisting}
.libPaths("/a/b/c/")
\end{lstlisting}

在一些情况下，由于所需库的位置原因，你可能需要手动安装一个 CRAN 上的扩展包。
这一点请参考下面的\ref{gpuinstall}节和\ref{rgpu}节。

{\bf 载入一个扩展包：}

通过调用 {\bf library()} 来载入一个扩展包。
例如，载入{\bf parallel}，可以使用：
\begin{lstlisting}
> library(parallel)
\end{lstlisting}

\section{R 中的 snow 扩展包}
\label{snow}

{\bf snow}最大的优点在于其简单。其概念和实现都非常简单，
能出错的地方不多。因此，它可能是现在使用最广泛的 R 并行包。


{\bf snow} 扩展包可以直接通过network socket运行
（由于用户只需要安装{\bf snow}，着可能是最常见的用法），
也可以运行于{\bf Rmpi}（R 的 MPI 接口）、PVM 或 NWS之上。

它也可以在一个 scatter/gather 模型（\ref{scattergather}节）下
进行操作。正如 R 中的{\bf apply()}函数会将同样的函数作用于
一个矩阵的每行上（见下面的示例），{\bf snow}中
的{\bf parApply()}会在多台机器上并行地完成类似的操作；
不同的机器会操作不同的行。（除了使用多台机器，我们也可以
在多核的机器上运行多个{\bf snow} client。）

\subsection{使用}

在使用
\begin{lstlisting}
> library(snow)
\end{lstlisting}
载入{\bf snow}之后，通过调用{\bf snow}中的{\bf makeCluster()}函数，
我们可以设置一个{\bf snow}集群。该函数的{\bf type}参数用于选择
网络平台，诸如``MPI''或``SOCK''。后者用于将{\bf snow}运行
于其自己创建的 TCP/IP sockets 之上，而不是使用 MPI 。

在这个例子里，我在名为{\bf pc48}和{\bf pc49}的电脑上使用``SOCK''选择，
以这种方式设置集群\footnote{如果你使用的是一个文件共享系统的电脑集群，
尽量保证 R 的安装路径一致，以避免问题。}：

\begin{lstlisting}
> cls <- makeCluster(type="SOCK",c("pc48","pc49"))
\end{lstlisting}

需要注意的是上面的 R 代码在名为{\bf pc48}和{\bf pc49}的机器上设置了
{\bf 工作节点}；这和{\bf 管理节点}相区别，管理节点运行于
执行 R 代码的机器上。

如果你想把工作节点和管理节点同时运行在同一台机器上（特别是在一台多核的机器上），
需要使用{\bf localhost}作为机器名。

还有其它很多可选的参数。一个你可能觉得非常有用的是{\bf outfile}，
它会把调用的结果记录在名为{\bf outfile}的文件里。
这在调用失败进行 debug 时非常有用。

\subsection{示例：使用 parApply() 进行矩阵向量相乘}

为了介绍{\bf snow}，让我们考虑一个简单的矩阵向量相乘
的简单示例。我是指一个测试矩阵如下：

\begin{lstlisting}[numbers=left]
> a <- matrix(c(1:12),nrow=6)
> a
     [,1] [,2]
[1,]    1    7
[2,]    2    8
[3,]    3    9
[4,]    4   10
[5,]    5   11
[6,]    6   12
\end{lstlisting}

我们会将向量 $(1,1)^{T}$ （T 这里表示转置）和矩阵相乘。
在这个简单的示例，我们当然可以直接完成：

\begin{lstlisting}
> a %*% c(1,1)
     [,1]
[1,]    8
[2,]   10
[3,]   12
[4,]   14
[5,]   16
[6,]   18
\end{lstlisting}

但是让我们看看如何使用 R 的{\bf apply()}来完成它。尽管这仍是
顺序执行，但这为我们扩展到并行计算提供了便利。

R 的{\bf apply()}函数调用一个用户定义的标量函数
作用于用户指定的矩阵的每一行（或每一列）。为了将{\bf apply()}用于
这里的矩阵向量相乘问题，我们定义一个点积的函数：

\begin{lstlisting}
> dot <- function(x,y) {return(x%*%y)}
\end{lstlisting}

现在调用{\bf apply()}：

\begin{lstlisting}
> apply(a,1,dot,c(1,1))
[1]  8 10 12 14 16 18
\end{lstlisting}

这个调用将函数{\bf dot()}作用于矩阵{\bf a}的每一行
（这个可以从1看出，2意味着每一列）；每一行都将作为
{\bf dot()}的第一个参数，而c(1,1)会作为第二个参数。
换言之，{\bf dot()}的第一次调用就是

\begin{lstlisting}
dot(c(1,7),c(1,1))
\end{lstlisting}

{\bf snow}中的{\bf parApply()}函数将{\bf apply()}扩展到并行计算。
我们把它用于将我们的矩阵相乘问题并行化，运行在我们名为{\bf cls}的集群之上：

\begin{lstlisting}
> parApply(cls,a,1,dot,c(1,1))
[1]  8 10 12 14 16 18
\end{lstlisting}

{\bf parApply()}所作的就是将矩阵每一行发送给每一个节点，
同时发送的还由函数{\bf dot()}和参数{\bf c(1,1)}。
每个节点将{\bf dot()}作用到接收的行上，之后将结果返回给管理节点。

R 的{\bf apply()}函数一般只用于变量值的情形，这意味着{\bf apply(m,i,f)}
调用中的函数{\bf f()}的返回值是标量。如果{\bf f()}的返回值是向量值，
那返回的会是一个矩阵而不是一个向量，矩阵里的每一列是
{\bf f()}作用于{\bf m}的一列或一行的结果。
{\bf parApply()}也同样如此。

\subsection{snow 中的其它函数：clusterApply()、clusterCall()等}

上一节，我们介绍了{\bf parApply()}函数。它可以这样调用

\begin{itemize}

\item {{\bf parApply()}:}

\begin{lstlisting}
parApply(cls,m,DIM,f,...)}
\end{lstlisting}

\end{itemize}

这个调用会把矩阵{\bf m}的每一行分配到{\bf cls}
的各个工作节点，之后函数{\bf f()}会被作用到每一行，
省略号在这里表示可选参数。参数{\bf DIM}为1时表示行操作，
2表示列操作。

返回值是一个向量（也可能是个矩阵，如上所述）。

{\bf snow}最大的有点在于其简单，因此并没有很多复杂的函数，
但当然不止{\bf parApply()}一个。这里列举了一些：

\begin{itemize}

\item {\bf clusterApply():}

这个函数可能是{\bf snow}中被使用最频繁的函数。

\begin{lstlisting}
clusterApply(cls,individualargs,f,...)}
\end{lstlisting}

这会使{\bf f()}在{\bf cls}中的每个节点上运行。这里的{\bf individualargs}
是一个 R 列表（如果是个向量，会被转换成列表）。当{\bf f()}在集群
中的节点 i 上被调用时，其参数如下所述：第一个参数
是{\bf individualargs}的第 i 个元素，或者说是{\bf individualargs[[i]]}；
如果在调用时，是用了省略号所代表的（可选）参数，它们会作为第二、第三或
更多的参数传递给{\bf f()}。

如果{\bf individualargs}的元素数量大于集群中的节点数，
那么{\bf cls}会被循环使用（可以把它作为一个向量对待），
所以多数或全部节点会在不止一个{\bf individualargs}元素
上调用{\bf f()}。返回值是一个 R 列表，其中第 i 个元素是{\bf f()}
作用于{\bf individualargs}中第 i 个元素的结果。
所以说，{\bf individualargs}列表又需要拆分并行计算的工作构成。

\item {\bf clusterApplyLB():}

这是{\bf clusterApply()}的负载均衡模式，
目的在于解决我们在第\ref{chap:issues}章中提到的性能问题。

为了解释{\bf clusterApply()}的两者形式的区别，
假设我们的集群由10个节点，而我们有25个需要执行的任务
（或者说{\bf individualargs}的长度是25）。如果使用{\bf clusterApply()}，
会发生下列这些：

\begin{itemize}

\item 前10个任务会被分配给工作节点，每个节点一个任务。

\item 管理节点会等这10个任务完成，之后再分配另外10个。

\item 管理节点会等这10个任务完成，之后在分配剩下的5个。

\item 管理节点会等这5个任务完成，之后返回25个结果。

\end{itemize}

而是用{\bf clusterApplyLB()}时，会按照下面这种方式执行：

\begin{itemize}

\item 前10个任务会被分配给工作节点，每个节点一个任务。

\item 当由节点任务结束时，管理节点会马上行动，将第11个任务分配
给这个节点，即使其它节点的任务还没完成。

\item 管理节点会继续照此工作，一旦一个节点任务完成，就会分配新的
任务，知道所有任务完成。

\item 管理节点最后会返回25个结果。

\end{itemize}

用第\ref{chap:issues}章和 OpenMP 一章中的\ref{schedulework}节的说法，
{\bf clusterApply()}使用了一种{\bf 静态}的调度策略，
而{\bf clusterApplyLB()}使用了一种动态策略；其中 chunk size 为1。

\item {\bf clusterCall()：}

函数{\bf clusterCall(cls,f,...)}将函数{\bf f()}和省略号所代表的参数（如果有的话），
发送到每个工作节点。在每个节点上，{\bf f()}会使用这些参数求值。
返回值是一个 R 列表，第 i 个元素师第 i 个节点的计算结果。
（一眼看上去，似乎每个节点都会返回同样的结果，但{\tt f()}会
使用每个节点特定的参数，从而返回不同的结果。）

\item {\bf clusterExport()：}

函数{\bf clusterExport(cls,varlist)}会将名字出现在字符向量{\bf varlist}
中的变量拷贝到{\bf cls}中的各个节点。你可以使用这个函数来
避免从管理节点到工作节点开销巨大的数据传输。
使用这个函数，你可以只传输数据集一次；
通过在相应的变量上使用{\bf clusterExport()}，之后在工作节点上
将其作为全局变量使用。
同样地，返回值仍是个 R 列表，第 i 个元素师集群中第 i 个节点的
计算结果。

默认情况下，被传输到工作节点的变量在管理节点上必须是全局变量。

需要特别注意的是，一旦你传输了一个变量，比如{\bf x}，
从管理节点到各个工作节点上，各个拷贝和工作节点上的变量就是独立的了
（各个拷贝之间也是相互独立的）。如果其中一个拷贝改变了，
在其他拷贝中不会反应这些变化。

\item {\bf clusterEvalQ()：}

函数{\bf clusterEvalQ(cls,expression)}会在{\bf cls}的各个节点
上运行{\bf expression}。

\end{itemize}

\subsection{示例：并行求和}

现在让我们再看一个示例，我们用{\bf snow}来进行并行求和。
先从一个很简单的版本开始，之后再考虑复杂的版本。

\begin{lstlisting}
parsum <- function(cls,x) {
   # `在节点上分配' x 的索引（实际上没有传输任何东西）
   xparts <- clusterSplit(cls,x)
   # 现在传输到节点上，并进行求和
   tmp <- clusterApply(cls,xparts,sum)
   # 现在将各个单独的加和合并得到结果
   tot <- 0
   for (i in 1:length(tmp)) tot <- tot + tmp[[i]]
   return(tot)
}
\end{lstlisting}

现在我们在一个有两个共走节点的集群{\bf cls}上进行测试：

\begin{lstlisting}
> x
[1]  1  2  3  4  5  6  5 12 13
> parsum1(cls,x)
[1] 51
\end{lstlisting}

结果不错。现在我们来想一下，这是如何完成的？

最基本的想法就是讲我们的向量分块，之后分配给工作节点。
每个工作节点会把所分配的小块求和，再把结果返回给管理节点。
管理节点会把这些结果求和，返回我们想要的求和的最终结果。

为了将我们的向量{\bf x}分块并发给各个节点，
我先来看{\bf snow}中的函数{\bf clusterSplit()}。
这个函数的输入是一个 R 向量，之后将其分块，分块的数量和
工作节点数相同。

例如，在上面的两个工作节点的集群上，我们得到：

\begin{lstlisting}
> xparts <- clusterSplit(cls,x)
> xparts
[[1]]
[1] 1 2 3 4

[[2]]
[1]  5  6  5 12 13
\end{lstlisting}

非常肯定的是，我们的列表{\bf xparts}有在其一个元素中有{\bf x}的一块，
而另一个元素中有{\bf x}的另一块。
之后这两块被传输到两个工作节点上：

\begin{lstlisting}
> tmp <- clusterApply(cls,xparts,sum)
> tmp
[[1]]
[1] 10

[[2]]
[1] 41
\end{lstlisting}

同样像{\bf snow}中的其他函数一样，{\bf clusterApply()}会以列表的
形式返回结果。这里我们将结果赋值给了{\bf tmp}。其内容如下
\begin{lstlisting}
> tmp
[[1]]
[1] 10

[[2]]
[1] 41
\end{lstlisting}
也就是{\bf x}每一小块的和。

为了得到最后的结果，我们不能简单地在{\bf tmp}上使用 R 中{\bf sum()}函数：

\begin{lstlisting}
> sum(tmp)
Error in sum(tmp) : invalid 'type' (list) of argument
\end{lstlisting}

这是因为{\bf sum()}接受的是向量，而不是列表。
所以我们自己写一个循环来把结果加起来：

\begin{lstlisting}
tot <- 0
for (i in 1:length(tmp)) tot <- tot + tmp[[i]]
\end{lstlisting}

需要注意的一点是，我们使用\texttt{[[]]}来获取列表中的元素。

我可以通过调用 R 中的{\bf Reduce()}函数来取代上面的循环，
从而对代码进行优化。{\bf Reduce()}很像\ref{ompreduction}节
和\ref{mpireduction}节中的 reduction 操作。（注意，这里是个串行
操作，不是并行。）一般以{\bf Reduce(f,y)}这种形式使用，
它对函数{\bf f()}和列表{\bf y}进行如下操作
\begin{lstlisting}
z <- y[1]
for (i in 2:length(y)) z <- f(z,y[i])
\end{lstlisting}

使用{\bf Reduce()}可以使代码更紧凑可读，
一些情况下还会提高执行效率（我们这里只有很少的
项目进行相加，暂时还不用考虑效率）。
而且，{\bf Reduce()}会将{\bf tmp}从一个列表转换为
向量，这就解决了我们直接对{\bf tmp}使用{\bf sum()}时
的问题。

下面是新的代码：

\begin{lstlisting}[numbers=left]
parsum <- function(cls,x) {
   xparts <- clusterSplit(cls,x)
   tmp <- clusterApply(cls,xparts,sum)
   Reduce(sum,tmp)  # implicit return()
}
\end{lstlisting}

需要说明的是，在 R 中，如果没有显式的{\bf return()}语句，
那最后求得的值会被作为返回值，这里是{\bf Reduce()}的计算结果。

{\bf Reduce()}是一个非常便利的函数，
特别是在和{\bf snow}一起使用时。
这里有一个我们把多个矩阵进行合并的示例：

\begin{lstlisting}
> Reduce(rbind,list(matrix(5:8,nrow=2),3:4,c(-1,1)))
     [,1] [,2]
[1,]    5    7
[2,]    6    8
[3,]    3    4
[4,]   -1    1
\end{lstlisting}

{\bf rbind()}函数只有两个参数，在这里我们有三个。
通过使用{\bf Reduce()}可以解决这个问题。

\subsection{示例：对角分块矩阵求逆}
\label{blkd}

假设我们有一个对角分块矩阵，比如
$$
\left (
   \begin{array}{cccc}
   1 & 2 & 0 & 0 \\
   3 & 4 & 0 & 0 \\
   0 & 0 & 8 & 1 \\
   0 & 0 & 1 & 5
   \end{array}
\right )
$$
我们想对其求逆。这是个易并行问题：假如我们有两个处理器，
我们可以很简单地让其中之一对第一个2$\times$2子矩阵求逆，
让另一个对第二个2$\times$2子矩阵求逆，之后我们将两个逆矩阵放回
原来的位置。

通讯的开销在这里不是很大，一个 n$\times$n 矩阵求逆的时间复杂度为$O(n^3)$，
而通讯只有$O(n^2)$。

现在我们讨论一下用于分块对角矩阵求逆的{\bf snow}代码。

\begin{lstlisting}
# invert a block diagonal matrix m, whose sizes are given in szs;
# return value is the inverted matrix
bdiaginv <- function(cls,m,szs) {
   nb <- length(szs)  # number of blocks
   dgs <- list()   # will form args for clusterApply()
   rownums <- getrng(szs)
   for (i in 1:nb) {
      rng <- rownums[i,1]:rownums[i,2]
      dgs[[i]] <- m[rng,rng]
   }
   invs <- clusterApply(cls,dgs,solve)
   for (i in 1:nb) {
      rng <- rownums[i,1]:rownums[i,2]
      m[rng,rng] <- invs[[i]]
   }
   m
}

# find row number ranges for the blocks, returned in a # 2-column
# matrix; blkszs = block sizes
getrng <- function(blkszs) {
   col2 <- cumsum(blkszs)  # cumulative sums function
   col1 <- col2 - (blkszs-1)
   cbind(col1,col2)  # column bind
}
\end{lstlisting}

我们来检测一下：

\begin{lstlisting}
> m
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    0    0    0
[2,]    7    8    0    0    0
[3,]    0    0    1    2    3
[4,]    0    0    2    4    5
[5,]    0    0    1    1    1
> bdiaginv(cls,m,c(2,3))
          [,1]       [,2] [,3] [,4] [,5]
[1,] -1.333333  0.3333333    0    0    0
[2,]  1.166667 -0.1666667    0    0    0
[3,]  0.000000  0.0000000    1   -1    2
[4,]  0.000000  0.0000000   -3    2   -1
[5,]  0.000000  0.0000000    2   -1    0
\end{lstlisting}

这里的{\bf szs}参数，包含了分块的大小。
由于我们只有一个2$\times$2和一个3$\times$3的块，分块的大小就是2和3，
因为在函数调用里使用{\bf c(2,3)}。

这里{\bf clusterApply()}的使用和早先的例子很相似。
代码中值得注意的地方是我们需要保存每一块在大矩阵
中的位置。最后我们写了一个{\bf getrng()}函数，用于返回
不同块的起始和结束的行数。我们通过使用这个函数来设置{\bf clusterApply()}
的{\bf dg}参数：

\begin{lstlisting}
for (i in 1:nb) {
   rng <- rownums[i,1]:rownums[i,2]
   dgs[[i]] <- m[rng,rng]
\end{lstlisting}

大家要记得表达式{\bf m[rng,rng]}会提取{\bf m}的行和列出来，
在这里就是第 i 块。

\subsection{示例：Mutual Outlink}
\label{rmutlinks}

让我们考虑\ref{mutlinks}节中的例子。
我们有一个网络，比如 web 链接。对于其中的两个节点，
比如两个网站，我可能对其 mutual outlink 感兴趣，
也就是两个网站共同的对外链接。

下面的{\bf snow}代码会计算整个网络中
任意一对节点 mutual outlink 的均值。

\begin{lstlisting}
# snow version of mutual links problem

library(snow)

mtl <- function(ichunks,m) {
   n <- ncol(m)
   matches <- 0
   for (i in ichunks) {
      if (i < n) {
         rowi <- m[i,]
         matches <- matches +
            sum(m[(i+1):n,] %*% as.vector(rowi))
      }
   }
   matches
}

# returns the mean number of mutual outlinks in m, computing on the
# cluster cls
mutlinks <- function(cls,m) {
   n <- nrow(m)
   nc <- length(cls)
   # determine which worker gets which chunk of i
   options(warn=-1)
   ichunks <- split(1:n,1:nc)
   options(warn=0)
   counts <- clusterApply(cls,ichunks,mtl,m)
   do.call(sum,counts) / (n*(n-1)/2)
}
\end{lstlisting}

对于{\bf m}中的每一行，我们会计算其下面每一行中的 mutual link。
为了在工作节点之间分配工作，我们可以如下使用{\bf clusterSplit()}
\begin{lstlisting}
clusterSplit(cls,1:nrow(m))
\end{lstlisting}

但这会有一个在\ref{mutlinks}节中讨论过的不均衡问题。
比如我们有两个工作节点和100行。
如果我们像上一节一样使用{\bf clusterSplit()}，
第一个节点进行的比较工作会远比第二个节点多。

一个解决方案是在调用{\bf clusterSplit()}之前，
将行号随机打乱。另一方法，也是我们上面的代码中使用的，
就是用 R 的{\bf split()}函数。

那{\bf split()}是做什么的？
它根据第二个参数中设置的``类别''，将第一个参数
进行分块处理。我们来看这个示例：

\begin{lstlisting}
> split(2:5,c('a','b'))
$a
[1] 2 4

$b
[1] 3 5
\end{lstlisting}

这里的种类是a和b。{\bf split()}函数要求第二个参数和第一个参数长度
相同，所以首先会对第二个参数进行``循环''处理成 a,b,a,b,a。
之后会将2和4放入类别 a，将3和5放入类别 b。
{\bf split()}函数最后会返回一个相应的列表。

现在我们再回到上面的{\bf snow}示例，我们仍然假设在
两个工作节点中分配100$\times$100的矩阵{\bf m}，代码
\begin{lstlisting}
nc <- length(cls)
ichunks <- split(1:n,1:nc)
\end{lstlisting}
会生成一个由两部分构成的列表，
第一部分由奇数行构成，第二部分由偶数行构成。
之后我们再使用
\begin{lstlisting}
counts <- clusterApply(cls,ichunks,mtl,m)
\end{lstlisting}
就可以在两个工作节点间实现一个负载均衡了。

注意这个调用需要将{\bf m}作为一个参数（作为函数{\bf mtl()}的参数）。
否则工作节点将没有可供使用的{\bf m}。
另一个选择是使用{\bf clusterExport()}来
将{\bf m}发送到工作节点，之后作为一个全局变量供{\bf mtl()}使用。

另外，调用{\bf options()}是为了让 R 在我们做``循环''时不
发出警告。一般我们并不这么做，但这里为了使用{\bf split()}的需要。

之后，为了得到输出的列表中各个元素的总和，我们可以再次使用{\bf Reduce()}，
但由于 R 的多样性，我们也可以使用{\bf do.call()}函数。
这个函数的动能正如其函数名暗示的：
它会把列表{\bf counts}中的每个元素抽出，之后作为参数传递给{\bf sum()}！
（一般来说，当我们需要调用一个特定的函数，但其参数的数目直到
运行时才可以确定时，{\bf do.call()}是非常有用的。）

正如前面所说的，除了使用{\bf split()}，我们可以将行数随机打乱：

\begin{lstlisting}
tmp <- clusterSplit(cls,order(runif(nrow(m))))
\end{lstlisting}

这会为每一行产生一个(0,1)之间的随机数，之后按此排序。
比如说，如果第三个随机数是第20小的，第三个元素在{\bf order()}的输出中会是20。
这可以找到矩阵{\bf m}行号的一个随机排列。

\subsection{示例：邻接矩阵变换}
\label{snowadj}

这是\ref{transgraph}节中代码的{\bf snow}版本。
回忆一下，问题如下：

假如我们有一个图的邻接矩阵
\begin{equation}
\left (
\begin{array}{rrrr}
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 1 \\
0 & 1 & 0 & 1 \\
1 & 1 & 1 & 0 \\
\end{array}
\right )
\end{equation}
其中行号和列号从0开始，而不是1。
我们想要将其转换为一个两列的矩阵用来展示连接数，
如下所示
\begin{equation}
\left (
\begin{array}{rr}
0 & 1 \\
1 & 0 \\
1 & 3 \\
2 & 1 \\
2 & 3 \\
3 & 0 \\
3 & 1 \\
3 & 2 \\
\end{array}
\right )
\end{equation}

比如说，在上面的邻接矩阵中，最右边的第二行有一个1，这意味
着在顶点1和顶点3直接存在一条边。
这个在转换后的矩阵中以(1,3)表示。

下面是在{\bf snow}中进行该计算的代码：

\begin{lstlisting}
tg <- function(cls,m) {
   n <- nrow(m)
   rowschunks <- clusterSplit(cls,1:n)  # make chunks of row numbers
   m1 <- cbind(1:n,m)  # prepend col of row numbers to m
   # now make the chunks of rows themselves
   tmp <- lapply(rowschunks,function(rchunk) m1[rchunk,])
   # launch the computation
   tmp <- clusterApply(cls,tmp,tgonchunk)
   do.call(rbind,tmp)  # combine into one large matrix
}

# a worker works on a chunk of rows
tgonchunk <- function(rows) {
   # note:  matrix space allocation not efficient
   mat <- NULL
   nc <- ncol(rows)
   for (i in 1:nrow(rows)) {
      row <- rows[i,]
      rownum <- row[1]
      for (j in 2:nc) {
         if (row[j] == 1) {
            if (is.null(mat)) {
               mat <- matrix(c(rownum,j-1),ncol=2)
            } else
               mat <- rbind(mat,c(rownum,j-1))
         }
      }
   }
   return(mat)
}
\end{lstlisting}

这里有什么新东西么？
首先，由于我们需要最后的输出按字典序排列，我们需要
保存原有每行的索引。所以我们需要在{\bf m}中多添加一列：

\begin{lstlisting}
m1 <- cbind(1:n,m)  # prepend col of row numbers to m
\end{lstlisting}

其次，注意{\bf lapply()}函数的使用。正如{\bf apply()}会
把一个特定的函数作用到矩阵的每一行（或每一列）上，
{\bf lapply()}会把一个特定函数作用到列表的每一个元素上。
输出结果依然是个列表。

在我们这里的例子中，我们需要将{\bf m}按行分块传递给{\bf clusterApply()}，
但后者要求我们必须传递一个列表。
我们可以通过一个{\bf for}来完成，一个一个地将分块添加进列表，
但使用{\bf lapply()}可以更加紧凑。

在最后，管理节点会接收新矩阵的很多部分，这些必须被整合起来。
使用{\bf rbind()}函数是很自然的想法，但我们仍然需要客服各个部分
是 R 列表的问题。尽管{\bf Reduce()}也可以完成，但用{\bf do.call()}会更趁手。

需要注意的是，尽管在上一段中说使用{\bf rbind()}是很自然的，
但效率很低。这是因为{\bf rbind()}会重新分配一个新的矩阵空间，
这是个很浪费时间的操作。先分配50行空间，之后在构建矩阵时进行填充会是更好的选择。
无论什么时候，我们用完了一个矩阵，我们都可以构建一个新的矩阵，
之后把所有矩阵作为一个列表返回。

\subsection{示例：设置节点 ID 和集群规模提示}

让我们回忆一下，在 OpenMP 中有两个函数，{\bf omp\_get\_thread\_num()}
和\\
{\bf omp\_get\_num\_threads()}，分别用来报告一个线程的 ID 和
线程总数。在 MPI 中，对应的函数时{\bf MPI\_Comm\_rank()}和{\bf MPI\_Comm\_size()}。
在{\bf snow}中如果能有这样的函数（或功能），将是非常好的事情。
这里的代码就是用于这个目的：

\begin{lstlisting}
# sets a list myinfo as a global variable in the worker nodes in the
# cluster cls, with myinfo$id being the ID number of the worker and
# myinfo$nwrkrs being the number of workers in the cluster; called from
# the manager node
setmyinfo <- function(cls) {
   setmyinfo <- function(i,n) {
      myinfo <<- list(id = i, nwrkrs = n)
   }
   ncls <- length(cls)
   clusterApply(cls,1:ncls,setmyinfo,ncls)
}
\end{lstlisting}

是的，R 允许在函数中定义函数。顺便请注意超级赋值符\verb#<<-#的使用，
这个用了在全局层面进行赋值操作。

调用这个函数后，任何在一个工作节点上运行的代码代码都
可以决定其节点 ID，比如在下面这样的代码中

\begin{lstlisting}
if (myinfo$id == 1) ...
\end{lstlisting}

或者，我们也可以从管理节点传输代码到工作节点执行：

\begin{lstlisting}
> setmyinfo(cls)
[[1]]
[[1]]$id
[1] 1

[[1]]$nwrkrs
[1] 2


[[2]]
[[2]]$id
[1] 2

[[2]]$nwrkrs
[1] 2

> clusterEvalQ(cls,myinfo$id)
[[1]]
[1] 1

[[2]]
[1] 2
\end{lstlisting}

第一个例子，由于{\bf clusterApply()}有返回值，都会被打印出来。
第二个例子中，调用
\begin{lstlisting}
clusterEvalQ(cls,myinfo$id)
\end{lstlisting}
会使每个工作节点对表达式\textbf{myinfo\$id}进行求值；
之后{\bf clusterEvalQ()}返回在每个节点上的执行结果。

\subsection{关闭集群}

退出 R 之前，不要忘记使用{\bf stopCluster(clustername)}来关闭集群。

\section{multicore 扩展包}

正如名字所暗示的，{\bf multicore}扩展包就是用于使用多核
设备的计算能力的。这可能有点奇怪：
既然{\bf snow}既可以用于一个（物理）集群，也可以
用于一个多核设备，而{\bf multicore}只能在后者上使用，
那用{\bf multicore}的优势哪儿？
答案是性能上的提高，这个在后面会解释。

这个扩展包的主函数是{\bf mclapply()}，其语法和
{\bf snow}中的{\bf clusterApply()}很类似，
也很类似地把任务分配给各个工作节点。

这里所说的工作节点，指的是同一台机器上的不同处理器。
比如说，在一个四核的机器上运行{\bf multicore}，
调用{\bf mclapply()}会（默认）在你的机器上调用4个 R，
并行地进行你的运算工作。其中每个 R 调用都使用和调用前的 R 一样的变量设置。
Thus
all the variables are shared initially (note that qualifier), and you as
the programmer do not take any special action to transfer variables from
the manager node to the worker nodes, quite a contrast to {\bf snow}.

The way all this is accomplished is that {\bf mclapply()} calls your OS'
{\bf fork()} function.  (It is thus limited to Unix-family OSs, such as
Linux and Macs.)  The process that is forked is R itself, with one new
copy per desired worker node.

The workers thus start with copies of R all sharing whatever variables
existed at the time of the fork (including locals in the function that
you had call {\bf mclapply()}).  Thus your code does not have to copy
these variables to the workers, which automatically have access to them.
But note carefully that the variables are shared only initially, and a
write to one copy is NOT reflected in the other copies (including the
original one).

The copying of the initial values of the variables from the manager node
to the worker nodes is done on a {\bf copy-on-write} basis, meaning that
data isn't copied to a node until (and unless) the node tries to access
that data.  The granularity is at the virtual memory page level (Section
\ref{howvmworks}).  Again, the OS handles this, not R.

Thus some physical copying does occur eventually, done by the OS, so
{\bf multicore} will not have as much advantage over {\bf snow} as one
might think.  However, there may be some latency-hiding advantage
(Section \ref{latencybandwidth}).  It may be the case that not all
workers need to access some variable at the same time, so one worker
might do so while others are doing actual computation.

Note too that, in contrast to {\bf snow}, in which a cluster is set up
once per session and then repeatedly reused at each {\bf snow} function
call, with {\bf multicore} the worker R processes are set up again from
scratch each time a {\bf multicore} function is called.

\subsection{示例：使用 multicore 进行邻接矩阵转换}

Same application as in Section \ref{snowadj}, and indeed the function
{\bf tgonchunk()} below is just a modified version of what we had in the
{\bf snow} code.

The call

\begin{lstlisting}
mclapply(starts,tgonchunk,m1,chunksize,mc.cores=ncores)
\end{lstlisting}

applies the function {\bf tgonchunk()} to every element of the vector
{\bf starts} (changed to an R list first), with {\bf m1} and {\bf
chunksize} serving as additional arguments to {\bf mclapply()}.

\begin{lstlisting}
# transgraph problem, R multicore version

# arguments:
#    m:  the input matrix
#    ncores:  desired number of cores to use
tgmc <- function(m,ncores) {
   n <- nrow(m)
   chunksize <- floor(n/ncores)
   starts <- seq(1,n,chunksize)
   m1 <- cbind(1:n,m)  # prepend col of row numbers to m
   tmp <- mclapply(starts,tgonchunk,m1,chunksize,mc.cores=ncores)
   do.call(rbind,tmp)
}

# a worker works on a chunk of rows
tgonchunk <- function(start,m1,chunksize) {
   # note:  matrix space allocation not efficient
   outmat <- NULL
   end <- start + chunksize - 1
   nrm <- nrow(m1)
   if (end > nrm) end <- nrm
   ncm <- ncol(m1)
   for (i in start:end) {
      rownum <- m1[i,1]
      for (j in 2:ncm) {
         if (m1[i,j] == 1) {
            if (is.null(outmat)) {
               outmat <- matrix(c(rownum,j-1),ncol=2)
            } else
               outmat <- rbind(outmat,c(rownum,j-1))
         }
      }
   }
   return(outmat)
}
\end{lstlisting}


\section{Rdsm}

My {\bf Rdsm} package can be used as a threads system regardless of
whether you are on a NOW or a multicore machine.  It is an extension of
a similar package I wrote in 2002 for Perl, called PerlDSM.  (N.
Matloff, PerlDSM: A Distributed Shared Memory System for Perl, {\it
Proceedings of PDPTA 2002}, 2002, 63-68.)  The major advantages of {\bf
Rdsm} are:

\begin{itemize}

\item It uses a shared-memory programming model, which as noted in
Section \ref{sharedbetter}, is commonly considered in the parallel
processing community to be clearer than messag-passing.

\item It allows full use of R's debugging tools.

\end{itemize}

{\bf Rdsm} gives the R programmer a shared memory view, but the objects
are not physically shared.  Instead, they are stored in a server and
accessed through network sockets,\footnote{Or, {\bf Rdsm} can be used
with the {\bf bigmemory} package, as seen in Section \ref{bigmemory}.}
thus enabling a threads-like view for R programmers even on NOWs.  There
is no manager/worker structure here.  All of the R processes execute the
same code, as peers.

Shared objects in {\bf Rdsm} can be numerical vectors or matrices, via
the classes {\bf dsmv} and {\bf dsmm}, or R lists, using the class {\bf
dsml}.  Communication with the server in the vector and matrix cases is
done in binary form for efficiency, while serialization is used for
lists.  There is as a built-in variable {\bf myinfo} that gives a
process' ID number and the total number of processes, analogous to the
information obtained in {\bf Rmpi} from the functions {\bf
mpi.comm.rank()} and {\bf mpi.comm.size()}.

To install, again use {\bf install.packages()} as above.  There is
built-in documentation, but it's best to read through the code {\bf
MatMul.R} in the {\bf examples} directory of the {\bf Rdsm} distribution
first.  It is heavily commented, with the goal of serving as an
introduction to the package.

\subsection{示例：使用 Rdsm 进行对角分块矩阵求逆}

Let's see how the block-diagonal matrix inversion example from Section
\ref{blkd} can be handled in {\bf Rdsm}.

\begin{lstlisting}
# invert a block diagonal matrix m, whose sizes are given in szs; here m
# is either an Rdsm or bigmemory shared variable; no return
# value--inversion is done in-place; it is assumed that there is one
# thread for each block

bdiaginv <- function(bd,szs) {
   # get number of rows of bd
   nrdb <- if(class(bd) == "big.matrix") dim(bd)[1] else bd$size[1]
   rownums <- getrng(nrdb,szs)
   myid <- myinfo$myid
   rng <- rownums[myid,1]:rownums[myid,2]
   bd[rng,rng] <- solve(bd[rng,rng])
   barr()  # barrier
}

# find row number ranges for the blocks, returned in a 2-column matrix;
# matsz = number of rows in matrix, blkszs = block sizes
getrng <- function(matsz, blkszs) {
   nb <- length(blkszs)
   rwnms <- matrix(nrow=nb,ncol=2)
   for (i in 1:nb) {
      # i-th block will be in rows (and cols)  i1:i2
      i1 <- if (i==1) 1 else i2 + 1
      i2 <- if (i == nb) matsz else i1 + blkszs[i] - 1
      rwnms[i,] <- c(i1,i2)
   }
   rwnms
}
\end{lstlisting}

The parallel work is basically done in four lines:

\begin{lstlisting}
myid <- myinfo$myid
rng <- rownums[myid,1]:rownums[myid,2]
bd[rng,rng] <- solve(bd[rng,rng])
barr()  # barrier
\end{lstlisting}

compared to about 11 lines in the {\bf snow} implementation above.  This
illustrates the power of the shared-memory programming model over
message passing.

\subsection{示例：Web Probe}

In the general programming community, one major class of applications,
even on a serial platform, is parallel I/O.  Since each I/O operation
may take a long time (by CPU standards), it makes sense to do them in
parallel if possible.  {\bf Rdsm} facilitates doing this in R.

The example below repeatedly cycles through a large list of Web sites,
taking measurements on the time to access each one.  The data are stored
in a shared variable {\bf accesstimes}; the {\bf n} most recent access
times are stored.  Each {\bf Rdsm} process works on one Web site at a
time.

An unusual feature here is that one of the processes immediately exits,
returning to the R interactive command line.  This allows the user to
monitor the data that is being collected.  Remember, the shared
variables are still accessible to that process.  Thus while the other
processes are continually adding data to {\bf accesstimes} (and deleted
one item for each one added), the user can give commands to the exited
process to analyze the data, say with histograms, as the collection
progresses.

Note the use of lock/unlock operations here, with the {\bf Rdsm}
variables of the same names.

\begin{lstlisting}
# if the variable accesstimes is length n, then the Rdsm vector
# accesstimes stores the n most recent probed access times, with element
# i being the i-th oldest

# arguments:
#    sitefile: IPs, one Web site per line
#    ww: window width, desired length of accesstimes
webprobe <- function(sitefile,ww) {
   # create shared variables
   cnewdsm("accesstimes","dsmv","double",rep(0,ww))
   cnewdsm("naccesstimes","dsmv","double",0)
   barr()  # Rdsm barrier
   # last thread is intended simply to provide access to humans, who
   # can do analyses on the data, typing commands, so have it exit this
   # function and return to the R command prompt
   # built-in R list myinfo has components to give thread ID number and
   # overall number of threads
   if (myinfo$myid == myinfo$nclnt) {
      print("back to R now")
      return()
   } else {  # the other processes continually probe the Web:
      sites <- scan(sitefile,what="")  # read from URL file
      nsites <- length(sites)
      repeat {
         # choose random site to probe
         site <- sites[sample(1:nsites,1)]
         # now probe it, recording the access time
         acc <- system.time(system(paste("wget --spider -q",site)))[3]
         # add to accesstimes, in sliding-window fashion
         lock("acclock")
         if (naccesstimes[1] < ww) {
            naccesstimes[1] <- naccesstimes[1] + 1
            accesstimes[naccesstimes[1]] <- acc
         } else {
            # out with the oldest, in with the newest
            newvec <- c(accesstimes[-1],acc)
            accesstimes[] <- newvec
         }
         unlock("acclock")
      }
   }
}
\end{lstlisting}

\subsection{bigmemory 扩展包}
\label{bigmemory}

Jay Emerson and Mike Kane developed the {\bf bigmemory} package when I
was developing {\bf Rdsm}; neither of us knew about the other.

The {\bf bigmemory} package is not intended to provide a threads
environment.  Instead, it is used to deal with a hard limit R has:
No R object can be larger than $2^{31}-1$ bytes.  This holds even if you
have a 64-bit machine with lots of RAM.  The {\bf bigmemory} package
solves the problem on a multicore machine, by making use of operating
system calls to set up shared memory between processes.\footnote{It can
also be used on distributed systems, by exploiting OS services to map
memory to files.}

In principle, {\bf bigmemory} could be used for threading, but the
package includes no infrastructure for this.  However, one can use {\bf
Rdsm} in conjunction with {\bf bigmemory}, an advantage since the latter
is very efficient.

Using {\bf bigmemory} variables in {\bf Rdsm} is quite simple:  Instead
of calling {\bf cnewdsm()} to create a shared variable, call {\bf newbm()}.

\section{R 和 GPU}

The blinding speed of GPUs (for certain problems) is sure to of interest
to more and more R users in the coming years.

As of today, the main vehicle for writing GPU code is CUDA, on NVIDIA
graphics cards.  CUDA is a slight extension of C.

You may need to write your own CUDA code, in which case you need to use
the methods of Section \ref{cfromr}.  But in many cases you can get what
you need in ready-made form, via the two main packages for GPU programming
with R, {\bf gputools} and {\bf rgpu}.  Both deal mainly with linear
algebra operations.  The remainder of this section will deal with these
packages.

\subsection{安装}
\label{gpuinstall}

Note that, due to issues involving linking to the CUDA libraries, in
the cases of these two packages, you probably will {\it not} be able to
install them by merely calling {\bf install.packages()}.   The
alternative I recommend works as follows:

\begin{itemize}

\item 下载{\bf .tar.gz}格式的扩展包。

\item 将扩展包解压缩，我们把产生的文件夹叫做{\bf x}。

\item 假设你想把它安装到{\bf /a/b/c}。

\item 对{\bf x}中的文件进行修改。

\item 之后运行 \texttt{R CMD INSTALL -l /a/b/c x}。

\end{itemize}

细节会在后面的章节中讨论。

\subsection{gputools 扩展包}

In installing {\bf gputools}, I downloaded the source from the CRAN R
repository site, and unpacked as above.  I then removed the subcommand

\begin{verbatim}
-gencode arch=compute_20,code=sm_20
\end{verbatim}

from the file {\bf Makefile.in} in the {\bf src} directory.  I also
made sure that my shell startup file included my CUDA executable and
library paths, {\bf /usr/local/cuda/bin} and {\bf /usr/local/cuda/lib}.

I then ran {\bf R CMD INSTALL} as above.  I tested it by trying
{\bf gpuLm.fit()}, the {\bf gputools} version of R's regular {\bf
lm.fit()}.

The package offers various linear algebra routines, such as matrix
multiplication, solution of Ax = b (and thus matrix inversion), and
singular value decomposition, as well as some computation-intensive
operations such as linear/generalize linear model estimation and
hierarchical clustering.

Here for instance is how to find the square of a matrix {\bf m}:

\begin{lstlisting}
> m2 <- gpuMatMult(m,m)
\end{lstlisting}

The {\bf gpuSolve()} function works like the R {\bf solve()}.  The call
\lstinline{gpuSolve(a,b)} will solve the linear system ax = b, for a
square matrix {\bf a} and vector {\bf b}.  If the second argument is
missing, then $a^{-1}$ will be returned.

\subsection{rgpu 扩展包}
\label{rgpu}

为了安装{\bf rgpu}，我从
\url{https://gforge.nbic.nl/frs/?group_id=38}下载源代码并解压缩。
之后我修改了{\bf Makefile}文件中的几行
\footnote{译者注：请根据自己机器上的相应路径进行修改}
\begin{lstlisting}
LIBS = -L/usr/lib/nvidia -lcuda -lcudart -lcublas
CUDA_INC_PATH = /home/matloff/NVIDIA_GPU_Computing_SDK/C/common/inc
R_INC_PATH = /usr/include/R
\end{lstlisting}

第一行是为了让系统找到{\bf -lcuda}，这点和{\bf gputools}一样。
第二行是为了 NVIDIA SDK 中的{\bf cutil.h}文件，上面的是我的安装路径。

For the third line, I made a file {\bf z.c} consisting solely of the
line

\begin{lstlisting}
#include <R.h>
\end{lstlisting}

and ran

\begin{lstlisting}
R CMD SHLIB z.c
\end{lstlisting}

just to see whether the R include file was.

As of May 2010, the routines in {\bf rgpu} are much less extensive than
those of {\bf gputools}.  However, one very nice feature of {\bf rgpu}
is that one can compute matrix expressions without bringing intermediate
results back from the device memory to the host memory, which would be a
big slowdown.  Here for instance is how to compute the square of the
matrix {\bf m}, plus itself:

\begin{lstlisting}
> m2m <- evalgpu(m %*% m + m)
\end{lstlisting}

\section{通过在 R 中调用 C 进行并行}
\label{cfromr}

Parallel R aims to be faster than ordinary R.  But even if that aim is
achieved, it's still R, and thus potentially slow.

One must always decide how much effort one is willing to devote to
optimization.  For the fastest code, we should not write in C, but
rather in assembly language.  Similarly, one must decide whether to
stick purely to R, or go to the faster C.  If parallel R gives you the
speed you need in your application, fine; if not, though, you should
consider writing part of your application in C, with the main part still
written in R.  You may find that placing the parallelism in the C
portion of your code is good enough, while retaining the convenience of
R for the rest of your code.

\subsection{在 R 中调用 C}

In C, two-dimensional arrays are stored in row-major order, in contrast
to R's column-major order.   For instance, if we have a 3x4 array, the
element in the second row and second column is element number 5 of the
array when viewed linearly, since there are three elements in the first
column and this is the second element in the second column.  Of course,
keep in mind that C subscripts begin at 0, rather than at 1 as with R.
In writing your C code to be interfaced to R, you must keep these issues
in mind.

All the arguments passed from R to C are received by C as pointers.
Note that the C function itself must return {\tt void}.  Values that we
would ordinarily return must in the R/C context be communicated through the
function's arguments, such as {\tt result} in our example below.

\subsection{Example:  Extracting Subdiagonals of a Matrix}

As an example, here is C code to extract subdiagonals from a square
matrix.\footnote{I wish to thank my former graduate assistant, Min-Yu
Huang, who wrote an earlier version of this function.} The code is in a
file {\bf sd.c}:

\begin{lstlisting}[numbers=left]
// arguments:
//    m:  a square matrix
//    n:  number of rows/columns of m
//    k:  the subdiagonal index--0 for main diagonal, 1 for first
//        subdiagonal, 2 for the second, etc.
//    result:  space for the requested subdiagonal, returned here

void subdiag(double *m, int *n, int *k, double *result)
{
  int nval = *n, kval = *k;
  int stride = nval + 1;
  for (int i = 0, j = kval; i < nval-kval; ++i, j+= stride)
     result[i] = m[j];
}
\end{lstlisting}

For convenience, you can compile this by rubnning R in a terminal
window, which will invoke GCC:

\begin{lstlisting}
% R CMD SHLIB sd.c
gcc -std=gnu99 -I/usr/share/R/include      -fpic  -g -O2 -c sd.c -o sd.o
gcc -std=gnu99 -shared  -o sd.so sd.o   -L/usr/lib/R/lib -lR
\end{lstlisting}

Note that here R showed us exactly what it did in invoking GCC.  This
allows us to do some customization.

But note that this simply produced a dynamic library, {\bf sd.o}, not an
executable program.  (On Windows this would presumably be a {\bf .dll}
file.)  So, how is it executed?  The answer is that it is loaded into R,
using R's {\bf dyn.load()} function.  Here is an example:

\begin{lstlisting}
> dyn.load("sd.so")
> m <- rbind(1:5, 6:10, 11:15, 16:20, 21:25)
> k <- 2
> .C("subdiag", as.double(m), as.integer(dim(m)[1]), as.integer(k),
result=double(dim(m)[1]-k))
[[1]]
 [1]  1  6 11 16 21  2  7 12 17 22  3  8 13 18 23  4  9 14 19 24  5 10 15 20 25

[[2]]
[1] 5

[[3]]
[1] 2

$result
[1] 11 17 23
\end{lstlisting}

Note that we needed to allocate space for {\tt result} in our call, in a
variable we've named {\tt result}.  The value placed in there by our
function is seen above to be correct.

\subsection{在 R 中调用 OpenMPI C 代码}

由于 OpenMP 可以由 C 使用，这就使得其可以从 R 中调用。
（关于 OpenMP 的详细讨论请见第\ref{chap:omp}章。）

在\ref{cfromr}节中类似，代码被编译并载入到 R 会话，
尽管有一些额外的步骤用于在调用 GCC 时设置{\tt -fopenmp}参数
（你需要手动运行，而不是使用{\bf R CMD SHLIB}）。

\subsection{在 R 中调用 CUDA 代码}

The same principles apply here, but one does have to be careful with
libraries and the like.

As before, we want to compile not to an executable file, but to a dynamic
library file.  Here's how, for the C file {\bf mutlinksforr.cu}
presented in the next section, the compile command is

\begin{lstlisting}
pc41:~% nvcc -g -G -I/usr/local/cuda/include -Xcompiler
   "-I/usr/include/R -fpic" -c mutlinksforr.cu -o mutlinks.o -arch=sm_11
pc41:~% nvcc -shared -Xlinker "-L/usr/lib/R/lib -lR"
   -L/usr/local/cuda/lib mutlinks.o -o meanlinks.so
\end{lstlisting}

The product of this was {\bf meanlinks.so}.  I then tested it on R:

\begin{lstlisting}
> dyn.load("meanlinks.so")
> m <- rbind(c(0,1,1,1),c(1,0,0,1),c(1,0,0,1),c(1,1,1,0))
> ma <- rbind(c(0,1,0),c(1,0,0),c(1,0,0))
> .C("meanout",as.integer(m),as.integer(4),mo=double(1))
[[1]]
 [1] 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0

[[2]]
[1] 4

$mo
[1] 1.333333

> .C("meanout",as.integer(ma),as.integer(3),mo=double(1))
[[1]]
[1] 0 1 1 1 0 0 0 0 0

[[2]]
[1] 3

$mo
[1] 0.3333333
\end{lstlisting}

\subsection{示例：Mutual Outlinks}

We again take as our example the mutual-outlinks example from Section
\ref{mutlinks}.  Here is an R/CUDA version:

\begin{lstlisting}[numbers=left]
// CUDA example:  finds mean number of mutual outlinks, among all pairs
// of Web sites in our set

#include <cuda.h>
#include <stdio.h>

// the following is needed to avoid variable name mangling
extern "C" void meanout(int *hm, int *nrc, double *meanmut);

// for a given thread number tn, calculates pair, the (i,j) to be
// processed by that thread; for nxn matrix
__device__ void findpair(int tn, int n, int *pair)
{  int sum=0,oldsum=0,i;
   for(i=0; ;i++) {
      sum += n - i - 1;
      if (tn <= sum-1) {
         pair[0] = i;
         pair[1] = tn - oldsum + i + 1;
         return;
      }
      oldsum = sum;
   }
}

// proc1pair() processes one pair of Web sites, i.e. one pair of rows in
// the nxn adjacency matrix m; the number of mutual outlinks is added to
// tot
__global__ void proc1pair(int *m, int *tot, int n)
{
   // find (i,j) pair to assess for mutuality
   int pair[2];
   findpair(threadIdx.x,n,pair);
   int sum=0;
   // make sure to account for R being column-major order; R's i-th row
   // is our i-th column here
   int startrowa = pair[0],
       startrowb = pair[1];
   for (int k = 0; k < n; k++)
      sum += m[startrowa + n*k] * m[startrowb + n*k];
   atomicAdd(tot,sum);
}

// meanout() is called from R
// hm points to the link matrix, nrc to the matrix size, meanmut to the output
void meanout(int *hm, int *nrc, double *meanmut)
{
    int n = *nrc,msize=n*n*sizeof(int);
    int *dm, // device matrix
        htot, // host grand total
        *dtot; // device grand total
    cudaMalloc((void **)&dm,msize);
    cudaMemcpy(dm,hm,msize,cudaMemcpyHostToDevice);
    htot = 0;
    cudaMalloc((void **)&dtot,sizeof(int));
    cudaMemcpy(dtot,&htot,sizeof(int),cudaMemcpyHostToDevice);
    dim3 dimGrid(1,1);
    int npairs = n*(n-1)/2;
    dim3 dimBlock(npairs,1,1);
    proc1pair<<<dimGrid,dimBlock>>>(dm,dtot,n);
    cudaThreadSynchronize();
    cudaMemcpy(&htot,dtot,sizeof(int),cudaMemcpyDeviceToHost);
    *meanmut = htot/double(npairs);
    cudaFree(dm);
    cudaFree(dtot);
}

\end{lstlisting}

The code is hardly optimal.  We should, for instance, have more than one
thread per block.

\section{调试 R 程序}

R 内置的调试机制是首选，在还存在着其它选择。

\subsection{文本编辑器}

然而，如果你是一个 Vim 编辑器的粉丝，我开发了一个可以极大扩展 R 调试器
的工具。请从 R 的 CRAN 上下载{\bf edtdbg}。
Emacs 中也有类似的工具。

Vitalie Spinu 的 {\tt ess-tracebug} 运行于 Emacs。
它大体基于{\tt edtdbg}，但提供了更多的针对 Emacs 的特性。

\subsection{IDE}

我个人不是提倡使用 IDE，但的确有一些很优秀的 IDE。

REvolution Analytics，一家提供 R 咨询和再开发版本 R 的公司，
他们提供了一个包含了很好的调试机制的 IDE。
但它只可以在 Windows 上运行，而且必须安装 Microsoft Visual Studio。

StatET，一个基于 Eclipse 的跨平台 IDE的开发者
在2011年五月添加了调试工具。


RStudio，另一个跨平台的 IDE的开发者，从2011年夏天也开始计划添加调试器
\footnote{译者注：RStudio 中的调试功能已添加}。


\subsection{缺少命令行终端的问题}

Parallel R packages such as {\bf Rmpi}, {\bf snow}, {\bf foreach} and so
on do not set up a terminal for each process, thus making it impossible
to use R's debugger on the workers.  What then can one do to debug apps
for those packages?  Let's consider {\bf snow} for concreteness.

First, one should debug the underlying single-worker function, such as {\bf
mtl()} in our mutual outlinks example in Section \ref{rmutlinks}.  Here
one would set up some artificial values of the arguments, and then use
R's ordinary debugging facilities.

This may be sufficient.  However, the bug may be in the arguments
themselves, or in the way we set them up.  Then things get more
difficult.  It's hard to even print out trace information, e.g. values
of variables, since {\bf print()} won't work in the worker processes.
The {\bf message()} function may work for some of these packages; if
not, you may have to resort to using {\bf cat()} to write to a file.

{\bf Rdsm} allows full debugging, as there is a separate terminal window
for each process.

\subsection{调试 R 所调用的 C 代码}

For parallel R that is implemented via R calls to C code, producing a
dynamically-loaded library as in Section \ref{cfromr}, debugging is a
little more involved.  First start R under GDB, then load the library to
be debugged.  At this point, R's interpreter will be looping,
anticipating reading an R command from you.  Break the loop by hitting
ctrl-c, which will put you back into {\it GDB's} interpreter.  Then set
a breakpoint at the C function you want to debug, say {\bf subdiag()} in
our example above.  Finally, tell GDB to continue, and it will then stop
in your function!  Here's how your session will look:

\begin{lstlisting}
$ R -d gdb
GNU gdb 6.8-debian
...
(gdb) run
Starting program: /usr/lib/R/bin/exec/R
...
> dyn.load("sd.so")
\end{lstlisting}

\section{本书中的其它 R 语言示例}

见下列章节中的示例（一些是非并行的）：

\ref{ompjacobi}节、\ref{onedimdft}节（非并行）和\ref{smoothing}节（非并行）。

\begin{itemize}

\item 线性等式的并行 Jacobi 迭代，\ref{ompjacobi}节。

\item 1维 FFT 的矩阵运算，\ref{onedimdft}节（可以通过并行的矩阵相乘来并行化）。

\item 2维 FFT 的并行计算，\ref{rfft}节。

\item 图像平滑，\ref{smoothing}节。

\end{itemize}



