\chapter{R 并行处理入门}
\label{chap:r}

\section{为什么要在本书中用 R 语言？}

在本书的其它章节里，C/C++依然是我们的主要语言，但我们也提供
很多 R 语言的示例。为什么要用 R 呢？

\begin{itemize}

\item R 是最广泛使用的用于统计分析和数据处理的编程语言。在现今
这个大数据时代，人民已经开发了相当数量的用于并行计算的 R 扩展包。
特别地，{\bf parallel} 扩展包现在已经是 R 基础包的一部分。

\item R 语言的广泛使用，从 Google 设置了其内部的 R 语言规范一事
就可见一斑\footnote{个人角度来讲，我并不喜欢这些代码规范，
我更喜欢我自己的。但从 Google 设置
自己的 R 语言规范可以看出他们对 R 的重视程度。}。现在 Oracle 也
把 R 包含了自己的大数据分析方案中。

\item 对于展示各种各样的并行算法，R 非常方便。这点的主要原因
在于 R 内置了向量、矩阵和复数类型。

\end{itemize}

Python 也有很多并行库，比如 {\bf multiprocessing}。关于 Python 的
并行话题，我们会在第\ref{chap:pythr}章里讨论。

{\bf 本章的示例会保持尽量简单。}但 R 中的并行计算也可以应用到非常庞大
而复杂的问题上。在附录\ref{chap:rquickstart}中，有一个5分钟的 R 快速入门。
阅读时请牢记 R 中 list 结构。

{\bf R 中进行并行计算的关键就是—— list 结构的操作。}许多 R 的并行计算
扩展包都非常依赖于 R 中的 list 结构。输入输出的参数和返回值经常
都采用 list 的形式。读者可能有
兴趣参考一下附录\ref{chap:rquickstart}中的相关内容。

\section{R 和易并行问题（Embarrassing Parallel Problems）}

需要注意的是，R 的并行扩展包一般只能处理易并行问题。正如在
\ref{embpar}节中定义的，这些问题不仅容易并行化，而且信息传递
的需求很少\footnote{后面的要求把很多迭代算法排除在外了，
尽管它们很容易并行化。}。如我们所知，一般只有易并行问题会有
很好的表现，但在 R 中情况尤其如此，原因如下。

R 语言的函数式编程的本质意味着，任何对一个向量或矩阵的元素的写入操作，
比如
\begin{lstlisting}
x[3] <- 8
\end{lstlisting}
都会重写整个向量或矩阵\footnote{R 中的元素赋值是一个函数调用，
上面这个例子的参数分别为 {\bf x}、3和8。}。虽然有些例外（随着 R 版本
更新，例外可能越来越多），但一般来说我们必须承认 R 中并行的向量
和矩阵代码代价很高\footnote{R 中新的引用类（Reference class）可能会对此有所改变。}。

对于不易并行的问题，大家应该考虑用 R 调用并行的 C 代码，这点会在\ref{cfromr}
节中讨论。

\section{一些 R 的并行扩展包}

这里我们列举了一些 R 的并行扩展包：

\begin{itemize}

\item Message-passing 或 scatter/gather (\ref{scattergather}节)：
{\bf Rmpi}、{\bf snow}、{\bf foreach}、{\bf rmr}、{\bf Rhipe}、{\bf multicore}\footnote{{\bf multicore} 扩展包运行于多核
内存共享的平台之上，但在读写过程中并不共享数据。}、{\bf rzmq}

\item 内存共享：{\bf Rdsm}、{\bf bigmemory}

\item GPU：{\bf gputools}、{\bf rgpu}

\end{itemize}

大家可以从
\url{http://cran.r-project.org/web/views/HighPerformanceComputing.html}找到更加详尽
的列表。

从2.14版本开始，R 默认包括了由 {\bf snow} 和 {\bf multicore} 构成
的 {\bf parallel} 扩展包。（早期版本可能需要分别下载。）
正是因为如此，二者都在范围之内。另外，我们也会讨论
{\bf Rdsm/bigmemory} 和 {\bf gputools}。

\section{安装和载入这些扩展包}

{\bf 安装：}

需要注意的是，如果你使用的是2.14版或更高版本的 R，你已经安装
了 {\bf snow} 和 {\bf multicore}。一般来说，除了 {\bf rgpu}，
其它所有扩展包都可以从 R 官方的代码仓库 CRAN\url{http://cran.r-project.org}）下载。
这里以 {\bf snow} 为例：

加入你想把它安装在 {\bf /a/b/c/} 目录下。最简单的方法就是
使用 R 的函数：

\begin{lstlisting}
> install.packages("snow","/a/b/c/")
\end{lstlisting}

这会将 {\bf snow} 安装在 {\bf /a/b/c/{\bf snow}} 目录下。

之后你需要将目录{\bf /a/b/c}（不是{\bf
/a/b/c/snow}）加到你的 R 搜索路径中。我推荐大家在
自己 home 目录下的{\bf .Rprofile}文件（这是 R 的启动设置文件）中添加这样一行。
\begin{lstlisting}
.libPaths("/a/b/c/")
\end{lstlisting}

在一些情况下，由于所需库的位置原因，你可能需要手动安装一个 CRAN 上的扩展包。
这一点请参考下面的\ref{gpuinstall}节和\ref{rgpu}节。

{\bf 载入一个扩展包：}

通过调用 {\bf library()} 来载入一个扩展包。
例如，载入{\bf parallel}，可以使用：
\begin{lstlisting}
> library(parallel)
\end{lstlisting}

\section{R 中的 snow 扩展包}
\label{snow}

{\bf snow}最大的优点在于其简单。其概念和实现都非常简单，
能出错的地方不多。因此，它可能是现在使用最广泛的 R 并行包。


{\bf snow} 扩展包可以直接通过network socket运行
（由于用户只需要安装{\bf snow}，着可能是最常见的用法），
也可以运行于{\bf Rmpi}（R 的 MPI 接口）、PVM 或 NWS之上。

它也可以在一个 scatter/gather 模型（\ref{scattergather}节）下
进行操作。正如 R 中的{\bf apply()}函数会将同样的函数作用于
一个矩阵的每行上（见下面的示例），{\bf snow}中
的{\bf parApply()}会在多台机器上并行地完成类似的操作；
不同的机器会操作不同的行。（除了使用多台机器，我们也可以
在多核的机器上运行多个{\bf snow} client。）

\subsection{使用}

在使用
\begin{lstlisting}
> library(snow)
\end{lstlisting}
载入{\bf snow}之后，通过调用{\bf snow}中的{\bf makeCluster()}函数，
我们可以设置一个{\bf snow}集群。该函数的{\bf type}参数用于选择
网络平台，诸如``MPI''或``SOCK''。后者用于将{\bf snow}运行
于其自己创建的 TCP/IP sockets 之上，而不是使用 MPI 。

在这个例子里，我在名为{\bf pc48}和{\bf pc49}的电脑上使用``SOCK''选择，
以这种方式设置集群\footnote{如果你使用的是一个文件共享系统的电脑集群，
尽量保证 R 的安装路径一致，以避免问题。}：

\begin{lstlisting}
> cls <- makeCluster(type="SOCK",c("pc48","pc49"))
\end{lstlisting}

需要注意的是上面的 R 代码在名为{\bf pc48}和{\bf pc49}的机器上设置了
{\bf 工作节点}；这和{\bf 管理节点}相区别，管理节点运行于
执行 R 代码的机器上。

如果你想把工作节点和管理节点同时运行在同一台机器上（特别是在一台多核的机器上），
需要使用{\bf localhost}作为机器名。

还有其它很多可选的参数。一个你可能觉得非常有用的是{\bf outfile}，
它会把调用的结果记录在名为{\bf outfile}的文件里。
这在调用失败进行 debug 时非常有用。

\subsection{示例：使用 parApply() 进行矩阵向量相乘}

为了介绍{\bf snow}，让我们考虑一个简单的矩阵向量相乘
的简单示例。我是指一个测试矩阵如下：

\begin{lstlisting}[numbers=left]
> a <- matrix(c(1:12),nrow=6)
> a
     [,1] [,2]
[1,]    1    7
[2,]    2    8
[3,]    3    9
[4,]    4   10
[5,]    5   11
[6,]    6   12
\end{lstlisting}

我们会将向量 $(1,1)^{T}$ （T 这里表示转置）和矩阵相乘。
在这个简单的示例，我们当然可以直接完成：

\begin{lstlisting}
> a %*% c(1,1)
     [,1]
[1,]    8
[2,]   10
[3,]   12
[4,]   14
[5,]   16
[6,]   18
\end{lstlisting}

但是让我们看看如何使用 R 的{\bf apply()}来完成它。尽管这仍是
顺序执行，但这为我们扩展到并行计算提供了便利。

R 的{\bf apply()}函数调用一个用户定义的标量函数
作用于用户指定的矩阵的每一行（或每一列）。为了将{\bf apply()}用于
这里的矩阵向量相乘问题，我们定义一个点积的函数：

\begin{lstlisting}
> dot <- function(x,y) {return(x%*%y)}
\end{lstlisting}

现在调用{\bf apply()}：

\begin{lstlisting}
> apply(a,1,dot,c(1,1))
[1]  8 10 12 14 16 18
\end{lstlisting}

这个调用将函数{\bf dot()}作用于矩阵{\bf a}的每一行
（这个可以从1看出，2意味着每一列）；每一行都将作为
{\bf dot()}的第一个参数，而c(1,1)会作为第二个参数。
换言之，{\bf dot()}的第一次调用就是

\begin{lstlisting}
dot(c(1,7),c(1,1))
\end{lstlisting}

{\bf snow}中的{\bf parApply()}函数将{\bf apply()}扩展到并行计算。
我们把它用于将我们的矩阵相乘问题并行化，运行在我们名为{\bf cls}的集群之上：

\begin{lstlisting}
> parApply(cls,a,1,dot,c(1,1))
[1]  8 10 12 14 16 18
\end{lstlisting}

{\bf parApply()}所作的就是将矩阵每一行发送给每一个节点，
同时发送的还由函数{\bf dot()}和参数{\bf c(1,1)}。
每个节点将{\bf dot()}作用到接收的行上，之后将结果返回给管理节点。

R 的{\bf apply()}函数一般只用于变量值的情形，这意味着{\bf apply(m,i,f)}
调用中的函数{\bf f()}的返回值是标量。如果{\bf f()}的返回值是向量值，
那返回的会是一个矩阵而不是一个向量，矩阵里的每一列是
{\bf f()}作用于{\bf m}的一列或一行的结果。
{\bf parApply()}也同样如此。

\subsection{snow 中的其它函数：clusterApply()、clusterCall()等}

上一节，我们介绍了{\bf parApply()}函数。它可以这样调用

\begin{itemize}

\item {{\bf parApply()}:}

\begin{lstlisting}
parApply(cls,m,DIM,f,...)}
\end{lstlisting}

\end{itemize}

这个调用会把矩阵{\bf m}的每一行分配到{\bf cls}
的各个工作节点，之后函数{\bf f()}会被作用到每一行，
省略号在这里表示可选参数。参数{\bf DIM}为1时表示行操作，
2表示列操作。

返回值是一个向量（也可能是个矩阵，如上所述）。

{\bf snow}最大的有点在于其简单，因此并没有很多复杂的函数，
但当然不止{\bf parApply()}一个。这里列举了一些：

\begin{itemize}

\item {\bf clusterApply():}

这个函数可能是{\bf snow}中被使用最频繁的函数。

\begin{lstlisting}
clusterApply(cls,individualargs,f,...)}
\end{lstlisting}

这会使{\bf f()}在{\bf cls}中的每个节点上运行。这里的{\bf individualargs}
是一个 R 列表（如果是个向量，会被转换成列表）。当{\bf f()}在集群
中的节点 i 上被调用时，其参数如下所述：第一个参数
是{\bf individualargs}的第 i 个元素，或者说是{\bf individualargs[[i]]}；
如果在调用时，是用了省略号所代表的（可选）参数，它们会作为第二、第三或
更多的参数传递给{\bf f()}。

如果{\bf individualargs}的元素数量大于集群中的节点数，
那么{\bf cls}会被循环使用（可以把它作为一个向量对待），
所以多数或全部节点会在不止一个{\bf individualargs}元素
上调用{\bf f()}。返回值是一个 R 列表，其中第 i 个元素是{\bf f()}
作用于{\bf individualargs}中第 i 个元素的结果。
所以说，{\bf individualargs}列表又需要拆分并行计算的工作构成。

\item {\bf clusterApplyLB():}

这是{\bf clusterApply()}的负载均衡模式，
目的在于解决我们在第\ref{chap:issues}章中提到的性能问题。

为了解释{\bf clusterApply()}的两者形式的区别，
假设我们的集群由10个节点，而我们有25个需要执行的任务
（或者说{\bf individualargs}的长度是25）。如果使用{\bf clusterApply()}，
会发生下列这些：

\begin{itemize}

\item 前10个任务会被分配给工作节点，每个节点一个任务。

\item 管理节点会等这10个任务完成，之后再分配另外10个。

\item 管理节点会等这10个任务完成，之后在分配剩下的5个。

\item 管理节点会等这5个任务完成，之后返回25个结果。

\end{itemize}

而是用{\bf clusterApplyLB()}时，会按照下面这种方式执行：

\begin{itemize}

\item 前10个任务会被分配给工作节点，每个节点一个任务。

\item 当由节点任务结束时，管理节点会马上行动，将第11个任务分配
给这个节点，即使其它节点的任务还没完成。

\item 管理节点会继续照此工作，一旦一个节点任务完成，就会分配新的
任务，知道所有任务完成。

\item 管理节点最后会返回25个结果。

\end{itemize}

用第\ref{chap:issues}章和 OpenMP 一章中的\ref{schedulework}节的说法，
{\bf clusterApply()}使用了一种{\bf 静态}的调度策略，
而{\bf clusterApplyLB()}使用了一种动态策略；其中 chunk size 为1。

\item {\bf clusterCall()：}

函数{\bf clusterCall(cls,f,...)}将函数{\bf f()}和省略号所代表的参数（如果有的话），
发送到每个工作节点。在每个节点上，{\bf f()}会使用这些参数求值。
返回值是一个 R 列表，第 i 个元素师第 i 个节点的计算结果。
（一眼看上去，似乎每个节点都会返回同样的结果，但{\tt f()}会
使用每个节点特定的参数，从而返回不同的结果。）

\item {\bf clusterExport()：}

函数{\bf clusterExport(cls,varlist)}会将名字出现在字符向量{\bf varlist}
中的变量拷贝到{\bf cls}中的各个节点。你可以使用这个函数来
避免从管理节点到工作节点开销巨大的数据传输。
使用这个函数，你可以只传输数据集一次；
通过在相应的变量上使用{\bf clusterExport()}，之后在工作节点上
将其作为全局变量使用。
同样地，返回值仍是个 R 列表，第 i 个元素师集群中第 i 个节点的
计算结果。

默认情况下，被传输到工作节点的变量在管理节点上必须是全局变量。

需要特别注意的是，一旦你传输了一个变量，比如{\bf x}，
从管理节点到各个工作节点上，各个拷贝和工作节点上的变量就是独立的了
（各个拷贝之间也是相互独立的）。如果其中一个拷贝改变了，
在其他拷贝中不会反应这些变化。

\item {\bf clusterEvalQ()：}

函数{\bf clusterEvalQ(cls,expression)}会在{\bf cls}的各个节点
上运行{\bf expression}。

\end{itemize}

\subsection{示例：并行求和}

现在让我们再看一个示例，我们用{\bf snow}来进行并行求和。
先从一个很简单的版本开始，之后再考虑复杂的版本。

\begin{lstlisting}
parsum <- function(cls,x) {
   # `在节点上分配' x 的索引（实际上没有传输任何东西）
   xparts <- clusterSplit(cls,x)
   # 现在传输到节点上，并进行求和
   tmp <- clusterApply(cls,xparts,sum)
   # 现在将各个单独的加和合并得到结果
   tot <- 0
   for (i in 1:length(tmp)) tot <- tot + tmp[[i]]
   return(tot)
}
\end{lstlisting}

现在我们在一个有两个共走节点的集群{\bf cls}上进行测试：

\begin{lstlisting}
> x
[1]  1  2  3  4  5  6  5 12 13
> parsum1(cls,x)
[1] 51
\end{lstlisting}

结果不错。现在我们来想一下，这是如何完成的？

The basic idea is to break our vector into chunks, then distribute the
chunks to the worker nodes.  Each of the latter will sum its chunk, and
send the sum back to the manager node.  The latter will sum the sums,
giving us the grand total as desired.

In order to break our vector {\bf x} into chunks to send to the workers,
we'll first turn to the {\bf snow} function {\bf clusterSplit()}.  That
function inputs an R vector and breaks it into as many chunks as we have
worker nodes, just what we want.

For example, with {\bf x} as above on a two-worker cluster, we get

\begin{lstlisting}
> xparts <- clusterSplit(cls,x)
> xparts
[[1]]
[1] 1 2 3 4

[[2]]
[1]  5  6  5 12 13
\end{lstlisting}

Sure enough, our R list {\bf xparts} has one chunk of {\bf x} in one of
its components, and the other chunk of {\bf x} in the other component.
These two chunks are now sent to our two worker nodes:

\begin{lstlisting}
> tmp <- clusterApply(cls,xparts,sum)
> tmp
[[1]]
[1] 10

[[2]]
[1] 41
\end{lstlisting}

Again, {\bf clusterApply()}, like most {\bf snow} functions, returns its
results in an R list, which we've assigned to {\bf tmp}.  The contents
of the latter are

\begin{lstlisting}
> tmp
[[1]]
[1] 10

[[2]]
[1] 41
\end{lstlisting}

i.e. the sum of each chunk of {\bf x}.

To get the grand total, we can't merely call R's {\bf sum()} function on
{\bf tmp}:

\begin{lstlisting}
> sum(tmp)
Error in sum(tmp) : invalid 'type' (list) of argument
\end{lstlisting}

This is because {\bf sum()} works on vectors, not lists.  So, we just
wrote a loop to add everything together:

\begin{lstlisting}
tot <- 0
for (i in 1:length(tmp)) tot <- tot + tmp[[i]]
\end{lstlisting}

Note that we need double brackets to access list elements.

We can improve the code a little by replacing the above loop code by a
call to R's {\bf Reduce()} function, which works like the reduction
operators we saw in Sections \ref{ompreduction} and \ref{mpireduction}.
(Note, though, that this is a serial operation here, not parallel.)
It takes the form {\bf Reduce(f,y)} for a function {\bf f()} and a list
{\bf y}, and essentially does

\begin{lstlisting}
z <- y[1]
for (i in 2:length(y)) z <- f(z,y[i])
\end{lstlisting}

Using {\bf Reduce()} makes for more compact, readable code, and in some
cases may speed up execution (not an issue here, since we'll have only a
few items to sum).  Moreover, {\bf Reduce()} changes {\bf tmp} from an R
list to a vector for us, solving the problem we had above when we tried
to apply {\bf sum()} to {\bf tmp} directly.

Here's the new code:

\begin{lstlisting}[numbers=left]
parsum <- function(cls,x) {
   xparts <- clusterSplit(cls,x)
   tmp <- clusterApply(cls,xparts,sum)
   Reduce(sum,tmp)  # implicit return()
}
\end{lstlisting}

Note that in R, absent an explcit {\bf return()} call, the last value
computed is returned, in this case the value produced by {\bf Reduce()}.

{\bf Reduce()} is a very handy function in R in general, and with {\bf
snow} in particular.  Here's an example in which we combine several
matrices into one:

\begin{lstlisting}
> Reduce(rbind,list(matrix(5:8,nrow=2),3:4,c(-1,1)))
     [,1] [,2]
[1,]    5    7
[2,]    6    8
[3,]    3    4
[4,]   -1    1
\end{lstlisting}

The {\bf rbind()} functions has two operands, but in the situation above
we have three.  Calling {\bf Reduce()} solves that problem.

\subsection{Example:  Inversion of Block-Diagonal Matrices}
\label{blkd}

Suppose we have a block-diagonal matrix, such as

$$
\left (
   \begin{array}{cccc}
   1 & 2 & 0 & 0 \\
   3 & 4 & 0 & 0 \\
   0 & 0 & 8 & 1 \\
   0 & 0 & 1 & 5
   \end{array}
\right )
$$

\noindent
and we wish to find its inverse.  This is an embarrassingly parallel
problem:  If we have two processes, we simply have one process invert that
first 2x2 submatrix, have the second process invert the second 2x2
submatrix, and we then place the inverses back in the same diagonal
positions.

Communication costs might not be too bad here, since inversion of an nxn
matrix takes $O(n^3)$ time while communication is only $O(n^2)$.

Here we'll discuss {\bf snow} code for inverting block-diagonal matrices.

\begin{lstlisting}[numbers=left]
# invert a block diagonal matrix m, whose sizes are given in szs;
# return value is the inverted matrix
bdiaginv <- function(cls,m,szs) {
   nb <- length(szs)  # number of blocks
   dgs <- list()   # will form args for clusterApply()
   rownums <- getrng(szs)
   for (i in 1:nb) {
      rng <- rownums[i,1]:rownums[i,2]
      dgs[[i]] <- m[rng,rng]
   }
   invs <- clusterApply(cls,dgs,solve)
   for (i in 1:nb) {
      rng <- rownums[i,1]:rownums[i,2]
      m[rng,rng] <- invs[[i]]
   }
   m
}

# find row number ranges for the blocks, returned in a # 2-column
# matrix; blkszs = block sizes
getrng <- function(blkszs) {
   col2 <- cumsum(blkszs)  # cumulative sums function
   col1 <- col2 - (blkszs-1)
   cbind(col1,col2)  # column bind
}
\end{lstlisting}

Let's test it:

\begin{lstlisting}
> m
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    0    0    0
[2,]    7    8    0    0    0
[3,]    0    0    1    2    3
[4,]    0    0    2    4    5
[5,]    0    0    1    1    1
> bdiaginv(cls,m,c(2,3))
          [,1]       [,2] [,3] [,4] [,5]
[1,] -1.333333  0.3333333    0    0    0
[2,]  1.166667 -0.1666667    0    0    0
[3,]  0.000000  0.0000000    1   -1    2
[4,]  0.000000  0.0000000   -3    2   -1
[5,]  0.000000  0.0000000    2   -1    0
\end{lstlisting}

Note the {\bf szs} argument here, which contains the sizes of the
blocks.  Since we had one 2x2 block and a 3x3 one, the sizes were 2 and
3, hence the {\bf c(2,3)} argument in our call.

The use of {\bf clusterApply()} here is similar to our earlier one.  The
main point in the code is to keep track of the positions of the blocks
within the big matrix.  To that end, we wrote {\bf getrng()}, which
returns the starting and ending row numbers for the various blocks.  we
use that to set up the argument {\bf dg} to be fed into {\bf
clusterApply()}:

\begin{lstlisting}
for (i in 1:nb) {
   rng <- rownums[i,1]:rownums[i,2]
   dgs[[i]] <- m[rng,rng]
\end{lstlisting}

Keep in mind that the expression {\bf m[rng,rng]} extracts a subset
of the rows and columns of {\bf m}, in this case the i$^{th}$ block.

\subsection{Example:  Mutual Outlinks}
\label{rmutlinks}

Consider the example of Section \ref{mutlinks}.  We have
a network graph of some kind, such as Web links.  For any two
vertices, say any two Web sites, we might be interested in mutual
outlinks, i.e. outbound links that are common to two Web sites.

The {\bf snow} code below finds the mean number of mutual outlinks, among
all pairs of sites in a set of Web sites.

\begin{lstlisting}[numbers=left]
# snow version of mutual links problem

library(snow)

mtl <- function(ichunks,m) {
   n <- ncol(m)
   matches <- 0
   for (i in ichunks) {
      if (i < n) {
         rowi <- m[i,]
         matches <- matches +
            sum(m[(i+1):n,] %*% as.vector(rowi))
      }
   }
   matches
}

# returns the mean number of mutual outlinks in m, computing on the
# cluster cls
mutlinks <- function(cls,m) {
   n <- nrow(m)
   nc <- length(cls)
   # determine which worker gets which chunk of i
   options(warn=-1)
   ichunks <- split(1:n,1:nc)
   options(warn=0)
   counts <- clusterApply(cls,ichunks,mtl,m)
   do.call(sum,counts) / (n*(n-1)/2)
}
\end{lstlisting}

For each row in {\bf m}, we will count mutual links in all rows below
that one.  To distribute the work among the worker nodes, we could have
a call to {\bf clusterSplit()} along the lines of

\begin{lstlisting}
clusterSplit(cls,1:nrow(m))
\end{lstlisting}

But this would presents a load imbalance problem, discussed in  Section
\ref{mutlinks}.  For instance, suppose again we have two worker nodes,
and there are 100 rows.  If we were to use {\bf clusterSplit()} as in
the last section, the first worker would be doing a lot more row
comparisons than would the second worker.

One solution to this problem would be to randomize the row numbers
before calling {\bf clusterSplit()}.  Another approach, taken in our
full code above, is to use R's {\bf split()} function.

What does {\bf split()} do?  It forms chunks of its first argument,
according to ``categories'' specified in the second.  Look at this
example:

\begin{lstlisting}
> split(2:5,c('a','b'))
$a
[1] 2 4

$b
[1] 3 5
\end{lstlisting}

Here the categories are 'a' and 'b'.  The {\bf split()} function
requires the second argument to be the same length as the first, so it
will first {\bf recycle} the second argument to 'a','b','a','b','a'.
The split will take 2,3,4,5 and treat 2 and 4 as being in categort 'a',
and 3 and 5 to be category 'b'.  The function returns a list
accordingly.

Now coming back to our above {\bf snow} example, and again assuming
two workers and  {\bf m} 100x100, the code

\begin{lstlisting}
nc <- length(cls)
ichunks <- split(1:n,1:nc)
\end{lstlisting}

produces a list of two components, with the odd-numbered rows in one
component and the evens in the other.  Our call,

\begin{lstlisting}
counts <- clusterApply(cls,ichunks,mtl,m)
\end{lstlisting}

then results in good load balance between the two workers.

Note that the call needed includ {\bf m} as an argument (which becomes
an argument to {\bf mtl()}).  Otherwise the workers would have no {\bf
m} to work it.  One alternative would have been to use {\bf
clusterExport()} to ship {\bf m} to the workers, at which it then would
be a global variable accessible by {\bf mtl()}.

By the way, the calls to {\bf options()} tell R not to warn us that it
did recycling.  It doesn't usually do so, but it will for {\bf split()}.

Then to get the grand total from the output list of individual sums, we
could have used {\bf Reduce()} again, but for variety utilized R's {\bf
do.call()} function.  That function does exactly what its name implies:
It will extract the elements of the list {\bf counts}, and then plug
them as arguments into {\bf sum()}!  (In general, {\bf do.call()} is
useful when we wish to call a certain function on a set of arguments
whose number won't be known until run time.)

As noted, instead of {\bf split()}, we could have randomized the rows:

\begin{lstlisting}
tmp <- clusterSplit(cls,order(runif(nrow(m))))
\end{lstlisting}

This generates a random number in (0,1) for each row, then finds the
order of these numbers.  If for instance the third number is the
20$^{th}$-smallest, element 3 of the output of {\bf order()} will be 20.
This amounts to finding a random permutation of the row numbers of {\bf
m}.

% \subsection{Example:  Doing a Scatter Operation}
%
% A common operation in message-passing parallel systems is {\bf
% scatter/gather} (Section \ref{scattergather}).  Actually, {\bf snow} is
% fundamentally a scatter/gather-based package, but the operation itself
% may take some doing.  Here's how we could set this up for matrices in
% {\bf snow}.
%
% Recall that {\bf clusterApply()} requires an R list as its first
% argument.  So, if for instance we wish to scatter the rows of a matrix,
% we'll need to first construct a list whose elements are those rows.
%
% Here's the code to do this:
%
% \begin{lstlisting}[numbers=left]
% # returns a list of the matrix m's rows (rowcol=1) or columns
% mat2lst <- function(m,rowcol=1) {
%    if (rowcol == 1) m <- t(m)
%    dm <- as.data.frame(m)
%    lapply(dm,function(col) col)
% }
% \end{lstlisting}
%
% Note the line
%
% \begin{lstlisting}
% lapply(dm,function(col) col)
% \end{lstlisting}
%
% The second argument in \lstinline{lapply()} needs to be a function.  It
% could be the name of a function, or a function defined ``on the spot''
% (like {\it anonymous} functions in, say, Python), as is the case here.
%
% On the R-help online discussion list, Petr Pikal suggested this
% approach:
%
% \begin{lstlisting}[numbers=left]
% # returns a list of the matrix m's rows
% mat2lst <- function(m) {
%    split(m, 1:nrow(m))
% }
% \end{lstlisting}
%
% This works because (a) an R matrix is actually a vector, store in
% row-major order and (b) in R a shorter vector is recycled to the length
% of the longer vector.
%
% This second approach seems to work faster for rows, as no transpose
% operation is done.
%
% Calling {\bf clusterApply()} with {\bf mrows}, the output of {\bf
% mat2lst(),} above would scatter the rows of the matrix to the various
% workers.  By the way, the workers may also need to know {\it which} rows
% they are given.  We could arrange that by appending a column to {\bf m}
% before calling {\bf mat2lst()}:
%
% \begin{lstlisting}
% m <- cbind(m,1:nrow(m))
% \end{lstlisting}

\subsection{Example:  Transforming an Adjacency Matrix}
\label{snowadj}

Here is a {\bf snow} version of the code in Section \ref{transgraph}.
To review, here is the problem:

Say we have a graph with adjacency matrix

\begin{equation}
\left (
\begin{array}{rrrr}
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 1 \\
0 & 1 & 0 & 1 \\
1 & 1 & 1 & 0 \\
\end{array}
\right )
\end{equation}

with row and column numbering starting at 0, not 1.  We'd like to
transform this to a two-column matrix that displays the links, in this
case

\begin{equation}
\left (
\begin{array}{rr}
0 & 1 \\
1 & 0 \\
1 & 3 \\
2 & 1 \\
2 & 3 \\
3 & 0 \\
3 & 1 \\
3 & 2 \\
\end{array}
\right )
\end{equation}

For instance, there is a 1 on the far right, second row of the above
matrix, meaning that in the graph there is an edge from vertex 1 to
vertex 3.  This results in the row (1,3) in the transformed matrix seen
above.

Here is code to do this computation in {\bf snow}:

\begin{lstlisting}[numbers=left]
tg <- function(cls,m) {
   n <- nrow(m)
   rowschunks <- clusterSplit(cls,1:n)  # make chunks of row numbers
   m1 <- cbind(1:n,m)  # prepend col of row numbers to m
   # now make the chunks of rows themselves
   tmp <- lapply(rowschunks,function(rchunk) m1[rchunk,])
   # launch the computation
   tmp <- clusterApply(cls,tmp,tgonchunk)
   do.call(rbind,tmp)  # combine into one large matrix
}

# a worker works on a chunk of rows
tgonchunk <- function(rows) {
   # note:  matrix space allocation not efficient
   mat <- NULL
   nc <- ncol(rows)
   for (i in 1:nrow(rows)) {
      row <- rows[i,]
      rownum <- row[1]
      for (j in 2:nc) {
         if (row[j] == 1) {
            if (is.null(mat)) {
               mat <- matrix(c(rownum,j-1),ncol=2)
            } else
               mat <- rbind(mat,c(rownum,j-1))
         }
      }
   }
   return(mat)
}
\end{lstlisting}

What is new here?  First, since we desired the output matrix to be in
lexicographical order, we needed a way to keep track of the original
indices of the rows.  So, we added a column for those numbers to {\bf
m}:

\begin{lstlisting}
m1 <- cbind(1:n,m)  # prepend col of row numbers to m
\end{lstlisting}

Second, note the use of R's {\bf lapply()} function.  Just as {\bf
apply()} calls a specified function on each row (or each column) of a
matrix, {\bf lapply()} calls a specified function on each element of a
list.  The output will also be a list.

In our case here, we need to feed the row chunks of {\bf m} into {\bf
clusterApply()}, but the latter requires that we do that via a list.
We could have done that using a {\bf for} loop, adding row chunks to a
list one by one, but it is more compact to use {\bf lapply()}.

In the end, the manager node receives many parts of the new matrix,
which must be combined.  It's natural to do that with the {\bf rbind()}
function, but again we need to overcome the fact that the parts are
packaged in an R list.  It's handy to use {\bf do.call()} again, though
{\bf Reduce()} would have worked too.

Note carefully that although it's natural to use {\bf rbind()} as
mentioned in the preceding paragraph, it's not efficient.  This is
because each call to {\bf rbind()} causes a new matrix to be allocated,
a time-consuming action.  It would be better to allocate, say, 50 rows
at a time, and fill in the rows as we build the matrix.  Whenever we
would use up all of a matrix, we would start a new one, and then return
all the matrices in a list.

\subsection{Example:  Setting Node IDs and Notification of Cluster Size}

Recall that in OpenMP there are functions {\bf omp\_get\_thread\_num()}
and {\bf omp\_get\_num\_threads()} that report a thread's ID number and
the total number of threads.  In MPI, the corresponding functions are
{\bf MPI\_Comm\_rank()} and {\bf MPI\_Comm\_size()}.  It would be nice
to have such functions (or such functionality) in {\bf snow}.  Here is
code for that purpose:

\begin{lstlisting}[numbers=left]
# sets a list myinfo as a global variable in the worker nodes in the
# cluster cls, with myinfo$id being the ID number of the worker and
# myinfo$nwrkrs being the number of workers in the cluster; called from
# the manager node
setmyinfo <- function(cls) {
   setmyinfo <- function(i,n) {
      myinfo <<- list(id = i, nwrkrs = n)
   }
   ncls <- length(cls)
   clusterApply(cls,1:ncls,setmyinfo,ncls)
}
\end{lstlisting}

Yes, R does allow defining a function within a function.  Note by the
way the use of the superassignment operator, \verb#<<-#, which assigns
to the global level.

After this call, any code executed by a worker node can then determine
its node number, e.g. in code such as

\begin{lstlisting}
if (myinfo$id == 1) ...
\end{lstlisting}

Or, we could send code from the manager to be executed on the workers:

\begin{lstlisting}
> setmyinfo(cls)
[[1]]
[[1]]$id
[1] 1

[[1]]$nwrkrs
[1] 2


[[2]]
[[2]]$id
[1] 2

[[2]]$nwrkrs
[1] 2

> clusterEvalQ(cls,myinfo$id)
[[1]]
[1] 1

[[2]]
[1] 2
\end{lstlisting}

In that first case, since {\bf clusterApply()} returns a value, it was
printed out.  In the second case, the call

\begin{lstlisting}
clusterEvalQ(cls,myinfo$id)
\end{lstlisting}

asks each worker to evaluate the expression \textbf{myinfo\$id};
{\bf clusterEvalQ()} then returns the results of evaluating the
expression at each worker node.

\subsection{Shutting Down a Cluster}

Don't forget to stop your clusters before exiting R, by calling
{\bf stopCluster(clustername)}.

\section{The multicore Package}

As the name implies, the {\bf multicore} package is used to exploit the
power of multicore machines.  This might seem odd: Since {\bf snow} can
be used on either a (physical) cluster of machines or on a multicore
machine, while {\bf multicore} can only be used on the latter, one might
wonder what, if anything, is to be gained by using {\bf multicore}.  The
answer is that one might gain in performance, as will be explained.

The package's main function, {\bf mclapply()}, is similar in syntax to
{\bf snow}'s {\bf clusterApply()}, and similarly parcels tasks out to
the various worker nodes.

The worker nodes here, though, are just different processes on the same
machine.  Say for example you are running {\bf multicore} on a quad-core
machine.  Calling {\bf mclapply()} will start (a default value of) 4 new
invocations of R on your machine, each of which will work on a piece of
your application in parallel.  Each invocation has exactly the same R
variables set up as your original R process did before the call.  Thus
all the variables are shared initially (note that qualifier), and you as
the programmer do not take any special action to transfer variables from
the manager node to the worker nodes, quite a contrast to {\bf snow}.

The way all this is accomplished is that {\bf mclapply()} calls your OS'
{\bf fork()} function.  (It is thus limited to Unix-family OSs, such as
Linux and Macs.)  The process that is forked is R itself, with one new
copy per desired worker node.

The workers thus start with copies of R all sharing whatever variables
existed at the time of the fork (including locals in the function that
you had call {\bf mclapply()}).  Thus your code does not have to copy
these variables to the workers, which automatically have access to them.
But note carefully that the variables are shared only initially, and a
write to one copy is NOT reflected in the other copies (including the
original one).

The copying of the initial values of the variables from the manager node
to the worker nodes is done on a {\bf copy-on-write} basis, meaning that
data isn't copied to a node until (and unless) the node tries to access
that data.  The granularity is at the virtual memory page level (Section
\ref{howvmworks}).  Again, the OS handles this, not R.

Thus some physical copying does occur eventually, done by the OS, so
{\bf multicore} will not have as much advantage over {\bf snow} as one
might think.  However, there may be some latency-hiding advantage
(Section \ref{latencybandwidth}).  It may be the case that not all
workers need to access some variable at the same time, so one worker
might do so while others are doing actual computation.

Note too that, in contrast to {\bf snow}, in which a cluster is set up
once per session and then repeatedly reused at each {\bf snow} function
call, with {\bf multicore} the worker R processes are set up again from
scratch each time a {\bf multicore} function is called.

\subsection{Example:  Transforming an Adjacency Matrix, multicore
Version}

Same application as in Section \ref{snowadj}, and indeed the function
{\bf tgonchunk()} below is just a modified version of what we had in the
{\bf snow} code.

The call

\begin{lstlisting}
mclapply(starts,tgonchunk,m1,chunksize,mc.cores=ncores)
\end{lstlisting}

applies the function {\bf tgonchunk()} to every element of the vector
{\bf starts} (changed to an R list first), with {\bf m1} and {\bf
chunksize} serving as additional arguments to {\bf mclapply()}.

\begin{lstlisting}[numbers=left]
# transgraph problem, R multicore version

# arguments:
#    m:  the input matrix
#    ncores:  desired number of cores to use
tgmc <- function(m,ncores) {
   n <- nrow(m)
   chunksize <- floor(n/ncores)
   starts <- seq(1,n,chunksize)
   m1 <- cbind(1:n,m)  # prepend col of row numbers to m
   tmp <- mclapply(starts,tgonchunk,m1,chunksize,mc.cores=ncores)
   do.call(rbind,tmp)
}

# a worker works on a chunk of rows
tgonchunk <- function(start,m1,chunksize) {
   # note:  matrix space allocation not efficient
   outmat <- NULL
   end <- start + chunksize - 1
   nrm <- nrow(m1)
   if (end > nrm) end <- nrm
   ncm <- ncol(m1)
   for (i in start:end) {
      rownum <- m1[i,1]
      for (j in 2:ncm) {
         if (m1[i,j] == 1) {
            if (is.null(outmat)) {
               outmat <- matrix(c(rownum,j-1),ncol=2)
            } else
               outmat <- rbind(outmat,c(rownum,j-1))
         }
      }
   }
   return(outmat)
}
\end{lstlisting}

% \section{Rmpi}
% \label{rmpi}
%
% The {\bf Rmpi} package provides an interface from R to MPI.  (MPI is
% covered in detail in Chapter \ref{chap:mpi}).  Its author is Hao Yu of
% the University of Western Ontario.
%
% It is arguably the most versatile of the parallel R packages, as it
% allows any node to communicate directly with any other node, without
% passing through a ``middleman.''  (The latter is the manager program in
% {\bf snow}, and the server in {\bf Rdsm}.)  This could enable major
% reductions in communications costs, and thus major increases in speed.
% Moreover, {\bf Rmpi} is more efficient in communication than is {\bf
% snow}, for example.
%
% So, although many applications using {\bf Rmpi} could be done more
% simply in {\bf snow}, the former can bring better performance.
%
% On the other hand, coding in {\bf Rmpi} generally requires more work
% than for the other packages.  In addition, MPI is quite finicky, and
% subtle errors in your setup may prevent it from running; that problem
% may be compounded if you run R and MPI together.  (In online R
% discussion groups, one of the most common types of queries concerns
% getting {\bf Rmpi} to run.)
%
% {\bf Rmpi} will not be used in this book, but here is an overview of how
% to use it:
%
% \subsection{Usage}
%
% Fire up MPI, and then from R load in {\bf Rmpi}, by typing
%
% \begin{lstlisting}
% > library(Rmpi)
% \end{lstlisting}
%
% Then start {\bf Rmpi}:
%
% \begin{lstlisting}
% > mpi.spawn.Rslaves()
% \end{lstlisting}
%
% On some systems, the call to {\bf mpi.spawn.Rslaves()} may encounter
% problems.  An alternate method of launching the worker processes is to
% copy the {\bf Rprofile} file in the {\bf Rmpi} distribution to {\bf
% .Rprofile} in your home directory.  Then start R, say for two workers
% and a manager, by running something like (for the LAM case)
%
% \begin{lstlisting}
% mpirun -c 3 R --no-save -q
% \end{lstlisting}
%
% This will start R on all machines in the group you started MPI on.
%
% \subsection{Available Functions}
%
% Most standard MPI functions are available, as well as many extras.
% Here are just a few examples:
%
% \begin{itemize}
%
% \item {\bf mpi.comm.size():}
%
% Returns the number of MPI processes, including the master that spawned
% the other processes.
%
% \item {\bf mpi.comm.rank():}
%
% Returns the rank of the process that executes it.
%
% \item {\bf mpi.send(), mpi.recv()}:
%
% The usual send/receive operations.
%
% \item {\bf mpi.bcast(), mpi.scatter(), mpi.gather():}
%
% The usual broadcast, scatter and gather operations.
%
% \end{itemize}



% \subsection{Example:  Inversion of a Diagonal-Block Matrix}
%
% Below is the {\bf Rmpi} code, for general n x n matrices of the
% block-diagonal form introduced in Section \ref{blkd}, but to keep this
% introductory Rmpi example simple, we'll assume that there are only two
% blocks, of the same size, with n/2 rows and n/2 columns, and with the
% manager itself doing half the work:
%
% \begin{lstlisting}[numbers=left]
% parinv <- function(blkdg) {
%    n <- nrow(blkdg)
%    k <- n/2  # block size
%    # send the worker its task function
%    mpi.bcast.Robj2slave(bkinv)
%    # get worker started running its task
%    mpi.bcast.cmd(bkinv())
%    # send worker data to feed its task
%    mpi.send.Robj(blkdg[(k+1):n,(k+1):n],dest=1,tag=1)
%    # prepare output matrix
%    outmat <- matrix(rep(0,n^2),nrow=n,ncol=n)
%    # manager does its task
%    outmat[1:k,1:k] <- solve(blkdg[1:k,1:k])
%    # receive result from worker and place in output matrix
%    outmat[(k+1):n,(k+1):n] <-
%       mpi.recv.Robj(source=1,tag=2)
%    return(outmat)
% }
%
% # function for worker to execute
% bkinv <- function() {
%    # receive data from manager
%    blk <- mpi.recv.Robj(source=0,tag=1)
%    # invert matrix and send back to manager
%    mpi.send.Robj(solve(blk),dest=0,tag=2)
% }
%
% test <- function() {
%    nb <- 800
%    nb1 <- nb+1
%    nbx2 <- 2*nb
%    blk <- matrix(runif(nb^2),nrow=nb)
%    mat <- matrix(rep(0,nbx2^2),nrow=nbx2)
%    mat[1:nb,1:nb] <- blk
%    mat[nb1:nbx2,nb1:nbx2] <- blk
%    print(system.time(parinv(mat)))
%    print(system.time(solve(mat)))
% }
% \end{lstlisting}

\section{Rdsm}

My {\bf Rdsm} package can be used as a threads system regardless of
whether you are on a NOW or a multicore machine.  It is an extension of
a similar package I wrote in 2002 for Perl, called PerlDSM.  (N.
Matloff, PerlDSM: A Distributed Shared Memory System for Perl, {\it
Proceedings of PDPTA 2002}, 2002, 63-68.)  The major advantages of {\bf
Rdsm} are:

\begin{itemize}

\item It uses a shared-memory programming model, which as noted in
Section \ref{sharedbetter}, is commonly considered in the parallel
processing community to be clearer than messag-passing.

\item It allows full use of R's debugging tools.

\end{itemize}

{\bf Rdsm} gives the R programmer a shared memory view, but the objects
are not physically shared.  Instead, they are stored in a server and
accessed through network sockets,\footnote{Or, {\bf Rdsm} can be used
with the {\bf bigmemory} package, as seen in Section \ref{bigmemory}.}
thus enabling a threads-like view for R programmers even on NOWs.  There
is no manager/worker structure here.  All of the R processes execute the
same code, as peers.

Shared objects in {\bf Rdsm} can be numerical vectors or matrices, via
the classes {\bf dsmv} and {\bf dsmm}, or R lists, using the class {\bf
dsml}.  Communication with the server in the vector and matrix cases is
done in binary form for efficiency, while serialization is used for
lists.  There is as a built-in variable {\bf myinfo} that gives a
process' ID number and the total number of processes, analogous to the
information obtained in {\bf Rmpi} from the functions {\bf
mpi.comm.rank()} and {\bf mpi.comm.size()}.

To install, again use {\bf install.packages()} as above.  There is
built-in documentation, but it's best to read through the code {\bf
MatMul.R} in the {\bf examples} directory of the {\bf Rdsm} distribution
first.  It is heavily commented, with the goal of serving as an
introduction to the package.

\subsection{Example:  Inversion of Block-Diagonal Matrices}

Let's see how the block-diagonal matrix inversion example from Section
\ref{blkd} can be handled in {\bf Rdsm}.

\begin{lstlisting}[numbers=left]
# invert a block diagonal matrix m, whose sizes are given in szs; here m
# is either an Rdsm or bigmemory shared variable; no return
# value--inversion is done in-place; it is assumed that there is one
# thread for each block

bdiaginv <- function(bd,szs) {
   # get number of rows of bd
   nrdb <- if(class(bd) == "big.matrix") dim(bd)[1] else bd$size[1]
   rownums <- getrng(nrdb,szs)
   myid <- myinfo$myid
   rng <- rownums[myid,1]:rownums[myid,2]
   bd[rng,rng] <- solve(bd[rng,rng])
   barr()  # barrier
}

# find row number ranges for the blocks, returned in a 2-column matrix;
# matsz = number of rows in matrix, blkszs = block sizes
getrng <- function(matsz, blkszs) {
   nb <- length(blkszs)
   rwnms <- matrix(nrow=nb,ncol=2)
   for (i in 1:nb) {
      # i-th block will be in rows (and cols)  i1:i2
      i1 <- if (i==1) 1 else i2 + 1
      i2 <- if (i == nb) matsz else i1 + blkszs[i] - 1
      rwnms[i,] <- c(i1,i2)
   }
   rwnms
}
\end{lstlisting}

The parallel work is basically done in four lines:

\begin{lstlisting}
myid <- myinfo$myid
rng <- rownums[myid,1]:rownums[myid,2]
bd[rng,rng] <- solve(bd[rng,rng])
barr()  # barrier
\end{lstlisting}

compared to about 11 lines in the {\bf snow} implementation above.  This
illustrates the power of the shared-memory programming model over
message passing.

\subsection{Example:  Web Probe}

In the general programming community, one major class of applications,
even on a serial platform, is parallel I/O.  Since each I/O operation
may take a long time (by CPU standards), it makes sense to do them in
parallel if possible.  {\bf Rdsm} facilitates doing this in R.

The example below repeatedly cycles through a large list of Web sites,
taking measurements on the time to access each one.  The data are stored
in a shared variable {\bf accesstimes}; the {\bf n} most recent access
times are stored.  Each {\bf Rdsm} process works on one Web site at a
time.

An unusual feature here is that one of the processes immediately exits,
returning to the R interactive command line.  This allows the user to
monitor the data that is being collected.  Remember, the shared
variables are still accessible to that process.  Thus while the other
processes are continually adding data to {\bf accesstimes} (and deleted
one item for each one added), the user can give commands to the exited
process to analyze the data, say with histograms, as the collection
progresses.

Note the use of lock/unlock operations here, with the {\bf Rdsm}
variables of the same names.

\begin{lstlisting}[numbers=left]
# if the variable accesstimes is length n, then the Rdsm vector
# accesstimes stores the n most recent probed access times, with element
# i being the i-th oldest

# arguments:
#    sitefile: IPs, one Web site per line
#    ww: window width, desired length of accesstimes
webprobe <- function(sitefile,ww) {
   # create shared variables
   cnewdsm("accesstimes","dsmv","double",rep(0,ww))
   cnewdsm("naccesstimes","dsmv","double",0)
   barr()  # Rdsm barrier
   # last thread is intended simply to provide access to humans, who
   # can do analyses on the data, typing commands, so have it exit this
   # function and return to the R command prompt
   # built-in R list myinfo has components to give thread ID number and
   # overall number of threads
   if (myinfo$myid == myinfo$nclnt) {
      print("back to R now")
      return()
   } else {  # the other processes continually probe the Web:
      sites <- scan(sitefile,what="")  # read from URL file
      nsites <- length(sites)
      repeat {
         # choose random site to probe
         site <- sites[sample(1:nsites,1)]
         # now probe it, recording the access time
         acc <- system.time(system(paste("wget --spider -q",site)))[3]
         # add to accesstimes, in sliding-window fashion
         lock("acclock")
         if (naccesstimes[1] < ww) {
            naccesstimes[1] <- naccesstimes[1] + 1
            accesstimes[naccesstimes[1]] <- acc
         } else {
            # out with the oldest, in with the newest
            newvec <- c(accesstimes[-1],acc)
            accesstimes[] <- newvec
         }
         unlock("acclock")
      }
   }
}
\end{lstlisting}

\subsection{The bigmemory Package}
\label{bigmemory}

Jay Emerson and Mike Kane developed the {\bf bigmemory} package when I
was developing {\bf Rdsm}; neither of us knew about the other.

The {\bf bigmemory} package is not intended to provide a threads
environment.  Instead, it is used to deal with a hard limit R has:
No R object can be larger than $2^{31}-1$ bytes.  This holds even if you
have a 64-bit machine with lots of RAM.  The {\bf bigmemory} package
solves the problem on a multicore machine, by making use of operating
system calls to set up shared memory between processes.\footnote{It can
also be used on distributed systems, by exploiting OS services to map
memory to files.}

In principle, {\bf bigmemory} could be used for threading, but the
package includes no infrastructure for this.  However, one can use {\bf
Rdsm} in conjunction with {\bf bigmemory}, an advantage since the latter
is very efficient.

Using {\bf bigmemory} variables in {\bf Rdsm} is quite simple:  Instead
of calling {\bf cnewdsm()} to create a shared variable, call {\bf newbm()}.

\section{R with GPUs}

The blinding speed of GPUs (for certain problems) is sure to of interest
to more and more R users in the coming years.

As of today, the main vehicle for writing GPU code is CUDA, on NVIDIA
graphics cards.  CUDA is a slight extension of C.

You may need to write your own CUDA code, in which case you need to use
the methods of Section \ref{cfromr}.  But in many cases you can get what
you need in ready-made form, via the two main packages for GPU programming
with R, {\bf gputools} and {\bf rgpu}.  Both deal mainly with linear
algebra operations.  The remainder of this section will deal with these
packages.

\subsection{Installation}
\label{gpuinstall}

Note that, due to issues involving linking to the CUDA libraries, in
the cases of these two packages, you probably will {\it not} be able to
install them by merely calling {\bf install.packages()}.   The
alternative I recommend works as follows:

\begin{itemize}

\item Download the package in {\bf .tar.gz} form.

\item Unpack the package, producing a directory that we'll call {\bf x}.

\item Let's say you wish to install to {\bf /a/b/c}.

\item Modify some files within {\bf x}.

\item Then run

\begin{lstlisting}
R CMD INSTALL -l /a/b/c x
\end{lstlisting}

\end{itemize}

Details will be shown in the following sections.

\subsection{The gputools Package}

In installing {\bf gputools}, I downloaded the source from the CRAN R
repository site, and unpacked as above.  I then removed the subcommand

\begin{verbatim}
-gencode arch=compute_20,code=sm_20
\end{verbatim}

from the file {\bf Makefile.in} in the {\bf src} directory.  I also
made sure that my shell startup file included my CUDA executable and
library paths, {\bf /usr/local/cuda/bin} and {\bf /usr/local/cuda/lib}.

I then ran {\bf R CMD INSTALL} as above.  I tested it by trying
{\bf gpuLm.fit()}, the {\bf gputools} version of R's regular {\bf
lm.fit()}.

The package offers various linear algebra routines, such as matrix
multiplication, solution of Ax = b (and thus matrix inversion), and
singular value decomposition, as well as some computation-intensive
operations such as linear/generalize linear model estimation and
hierarchical clustering.

Here for instance is how to find the square of a matrix {\bf m}:

\begin{lstlisting}
> m2 <- gpuMatMult(m,m)
\end{lstlisting}

The {\bf gpuSolve()} function works like the R {\bf solve()}.  The call
\lstinline{gpuSolve(a,b)} will solve the linear system ax = b, for a
square matrix {\bf a} and vector {\bf b}.  If the second argument is
missing, then $a^{-1}$ will be returned.

\subsection{The rgpu Package}
\label{rgpu}

In installing {\bf rgpu}, I downloaded the source code from
\url{https://gforge.nbic.nl/frs/?group_id=38} and unpacked as above.
I then changed the file {\bf Makefile}, with the modified lines being

\begin{lstlisting}[numbers=left]
LIBS = -L/usr/lib/nvidia -lcuda -lcudart -lcublas
CUDA_INC_PATH ?= /home/matloff/NVIDIA_GPU_Computing_SDK/C/common/inc
R_INC_PATH ?= /usr/include/R
\end{lstlisting}

The first line was needed to pick up {\bf -lcuda}, as with {\bf
gputools}.  The second line was needed to acquire the file {\bf cutil.h}
in the NVIDIA SDK, which I had installed earlier at the location see
above.

For the third line, I made a file {\bf z.c} consisting solely of the
line

\begin{lstlisting}
#include <R.h>
\end{lstlisting}

and ran

\begin{lstlisting}
R CMD SHLIB z.c
\end{lstlisting}

just to see whether the R include file was.

As of May 2010, the routines in {\bf rgpu} are much less extensive than
those of {\bf gputools}.  However, one very nice feature of {\bf rgpu}
is that one can compute matrix expressions without bringing intermediate
results back from the device memory to the host memory, which would be a
big slowdown.  Here for instance is how to compute the square of the
matrix {\bf m}, plus itself:

\begin{lstlisting}
> m2m <- evalgpu(m %*% m + m)
\end{lstlisting}

\section{Parallelism Via Calling C from R}
\label{cfromr}

Parallel R aims to be faster than ordinary R.  But even if that aim is
achieved, it's still R, and thus potentially slow.

One must always decide how much effort one is willing to devote to
optimization.  For the fastest code, we should not write in C, but
rather in assembly language.  Similarly, one must decide whether to
stick purely to R, or go to the faster C.  If parallel R gives you the
speed you need in your application, fine; if not, though, you should
consider writing part of your application in C, with the main part still
written in R.  You may find that placing the parallelism in the C
portion of your code is good enough, while retaining the convenience of
R for the rest of your code.

\subsection{Calling C from R}

In C, two-dimensional arrays are stored in row-major order, in contrast
to R's column-major order.   For instance, if we have a 3x4 array, the
element in the second row and second column is element number 5 of the
array when viewed linearly, since there are three elements in the first
column and this is the second element in the second column.  Of course,
keep in mind that C subscripts begin at 0, rather than at 1 as with R.
In writing your C code to be interfaced to R, you must keep these issues
in mind.

All the arguments passed from R to C are received by C as pointers.
Note that the C function itself must return {\tt void}.  Values that we
would ordinarily return must in the R/C context be communicated through the
function's arguments, such as {\tt result} in our example below.

\subsection{Example:  Extracting Subdiagonals of a Matrix}

As an example, here is C code to extract subdiagonals from a square
matrix.\footnote{I wish to thank my former graduate assistant, Min-Yu
Huang, who wrote an earlier version of this function.} The code is in a
file {\bf sd.c}:

\begin{lstlisting}[numbers=left]
// arguments:
//    m:  a square matrix
//    n:  number of rows/columns of m
//    k:  the subdiagonal index--0 for main diagonal, 1 for first
//        subdiagonal, 2 for the second, etc.
//    result:  space for the requested subdiagonal, returned here

void subdiag(double *m, int *n, int *k, double *result)
{
  int nval = *n, kval = *k;
  int stride = nval + 1;
  for (int i = 0, j = kval; i < nval-kval; ++i, j+= stride)
     result[i] = m[j];
}
\end{lstlisting}

For convenience, you can compile this by rubnning R in a terminal
window, which will invoke GCC:

\begin{lstlisting}[numbers=left]
% R CMD SHLIB sd.c
gcc -std=gnu99 -I/usr/share/R/include      -fpic  -g -O2 -c sd.c -o sd.o
gcc -std=gnu99 -shared  -o sd.so sd.o   -L/usr/lib/R/lib -lR
\end{lstlisting}

Note that here R showed us exactly what it did in invoking GCC.  This
allows us to do some customization.

But note that this simply produced a dynamic library, {\bf sd.o}, not an
executable program.  (On Windows this would presumably be a {\bf .dll}
file.)  So, how is it executed?  The answer is that it is loaded into R,
using R's {\bf dyn.load()} function.  Here is an example:

\begin{lstlisting}[numbers=left]
> dyn.load("sd.so")
> m <- rbind(1:5, 6:10, 11:15, 16:20, 21:25)
> k <- 2
> .C("subdiag", as.double(m), as.integer(dim(m)[1]), as.integer(k),
result=double(dim(m)[1]-k))
[[1]]
 [1]  1  6 11 16 21  2  7 12 17 22  3  8 13 18 23  4  9 14 19 24  5 10 15 20 25

[[2]]
[1] 5

[[3]]
[1] 2

$result
[1] 11 17 23
\end{lstlisting}

Note that we needed to allocate space for {\tt result} in our call, in a
variable we've named {\tt result}.  The value placed in there by our
function is seen above to be correct.

\subsection{在 R 中调用 OpenMPI C 代码}

由于 OpenMP 可以由 C 使用，这就使得其可以从 R 中调用。
（关于 OpenMP 的详细讨论请见第\ref{chap:omp}章。）

在\ref{cfromr}节中类似，代码被编译并载入到 R 会话，
尽管有一些额外的步骤用于在调用 GCC 时设置{\tt -fopenmp}参数
（你需要手动运行，而不是使用{\bf R CMD SHLIB}）。

\subsection{在 R 中调用 CUDA 代码}

The same principles apply here, but one does have to be careful with
libraries and the like.

As before, we want to compile not to an executable file, but to a dynamic
library file.  Here's how, for the C file {\bf mutlinksforr.cu}
presented in the next section, the compile command is

\begin{lstlisting}
pc41:~% nvcc -g -G -I/usr/local/cuda/include -Xcompiler
   "-I/usr/include/R -fpic" -c mutlinksforr.cu -o mutlinks.o -arch=sm_11
pc41:~% nvcc -shared -Xlinker "-L/usr/lib/R/lib -lR"
   -L/usr/local/cuda/lib mutlinks.o -o meanlinks.so
\end{lstlisting}

The product of this was {\bf meanlinks.so}.  I then tested it on R:

\begin{lstlisting}
> dyn.load("meanlinks.so")
> m <- rbind(c(0,1,1,1),c(1,0,0,1),c(1,0,0,1),c(1,1,1,0))
> ma <- rbind(c(0,1,0),c(1,0,0),c(1,0,0))
> .C("meanout",as.integer(m),as.integer(4),mo=double(1))
[[1]]
 [1] 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0

[[2]]
[1] 4

$mo
[1] 1.333333

> .C("meanout",as.integer(ma),as.integer(3),mo=double(1))
[[1]]
[1] 0 1 1 1 0 0 0 0 0

[[2]]
[1] 3

$mo
[1] 0.3333333
\end{lstlisting}

\subsection{示例：Mutual Outlinks}

We again take as our example the mutual-outlinks example from Section
\ref{mutlinks}.  Here is an R/CUDA version:

\begin{lstlisting}[numbers=left]
// CUDA example:  finds mean number of mutual outlinks, among all pairs
// of Web sites in our set

#include <cuda.h>
#include <stdio.h>

// the following is needed to avoid variable name mangling
extern "C" void meanout(int *hm, int *nrc, double *meanmut);

// for a given thread number tn, calculates pair, the (i,j) to be
// processed by that thread; for nxn matrix
__device__ void findpair(int tn, int n, int *pair)
{  int sum=0,oldsum=0,i;
   for(i=0; ;i++) {
      sum += n - i - 1;
      if (tn <= sum-1) {
         pair[0] = i;
         pair[1] = tn - oldsum + i + 1;
         return;
      }
      oldsum = sum;
   }
}

// proc1pair() processes one pair of Web sites, i.e. one pair of rows in
// the nxn adjacency matrix m; the number of mutual outlinks is added to
// tot
__global__ void proc1pair(int *m, int *tot, int n)
{
   // find (i,j) pair to assess for mutuality
   int pair[2];
   findpair(threadIdx.x,n,pair);
   int sum=0;
   // make sure to account for R being column-major order; R's i-th row
   // is our i-th column here
   int startrowa = pair[0],
       startrowb = pair[1];
   for (int k = 0; k < n; k++)
      sum += m[startrowa + n*k] * m[startrowb + n*k];
   atomicAdd(tot,sum);
}

// meanout() is called from R
// hm points to the link matrix, nrc to the matrix size, meanmut to the output
void meanout(int *hm, int *nrc, double *meanmut)
{
    int n = *nrc,msize=n*n*sizeof(int);
    int *dm, // device matrix
        htot, // host grand total
        *dtot; // device grand total
    cudaMalloc((void **)&dm,msize);
    cudaMemcpy(dm,hm,msize,cudaMemcpyHostToDevice);
    htot = 0;
    cudaMalloc((void **)&dtot,sizeof(int));
    cudaMemcpy(dtot,&htot,sizeof(int),cudaMemcpyHostToDevice);
    dim3 dimGrid(1,1);
    int npairs = n*(n-1)/2;
    dim3 dimBlock(npairs,1,1);
    proc1pair<<<dimGrid,dimBlock>>>(dm,dtot,n);
    cudaThreadSynchronize();
    cudaMemcpy(&htot,dtot,sizeof(int),cudaMemcpyDeviceToHost);
    *meanmut = htot/double(npairs);
    cudaFree(dm);
    cudaFree(dtot);
}

\end{lstlisting}

The code is hardly optimal.  We should, for instance, have more than one
thread per block.

\section{调试 R 程序}

R 内置的调试机制是首选，在还存在着其它选择。

\subsection{文本编辑器}

然而，如果你是一个 Vim 编辑器的粉丝，我开发了一个可以极大扩展 R 调试器
的工具。请从 R 的 CRAN 上下载{\bf edtdbg}。
Emacs 中也有类似的工具。

Vitalie Spinu 的 {\tt ess-tracebug} 运行于 Emacs。
它大体基于{\tt edtdbg}，但提供了更多的针对 Emacs 的特性。

\subsection{IDE}

我个人不是提倡使用 IDE，但的确有一些很优秀的 IDE。

REvolution Analytics，一家提供 R 咨询和再开发版本 R 的公司，
他们提供了一个包含了很好的调试机制的 IDE。
但它只可以在 Windows 上运行，而且必须安装 Microsoft Visual Studio。

StatET，一个基于 Eclipse 的跨平台 IDE的开发者
在2011年五月添加了调试工具。


RStudio，另一个跨平台的 IDE的开发者，从2011年夏天也开始计划添加调试器
\footnote{译者注：RStudio 中的调试功能已添加}。


\subsection{缺少命令行终端的问题}

Parallel R packages such as {\bf Rmpi}, {\bf snow}, {\bf foreach} and so
on do not set up a terminal for each process, thus making it impossible
to use R's debugger on the workers.  What then can one do to debug apps
for those packages?  Let's consider {\bf snow} for concreteness.

First, one should debug the underlying single-worker function, such as {\bf
mtl()} in our mutual outlinks example in Section \ref{rmutlinks}.  Here
one would set up some artificial values of the arguments, and then use
R's ordinary debugging facilities.

This may be sufficient.  However, the bug may be in the arguments
themselves, or in the way we set them up.  Then things get more
difficult.  It's hard to even print out trace information, e.g. values
of variables, since {\bf print()} won't work in the worker processes.
The {\bf message()} function may work for some of these packages; if
not, you may have to resort to using {\bf cat()} to write to a file.

{\bf Rdsm} allows full debugging, as there is a separate terminal window
for each process.

\subsection{Debugging C Called from R}

For parallel R that is implemented via R calls to C code, producing a
dynamically-loaded library as in Section \ref{cfromr}, debugging is a
little more involved.  First start R under GDB, then load the library to
be debugged.  At this point, R's interpreter will be looping,
anticipating reading an R command from you.  Break the loop by hitting
ctrl-c, which will put you back into {\it GDB's} interpreter.  Then set
a breakpoint at the C function you want to debug, say {\bf subdiag()} in
our example above.  Finally, tell GDB to continue, and it will then stop
in your function!  Here's how your session will look:

\begin{lstlisting}
$ R -d gdb
GNU gdb 6.8-debian
...
(gdb) run
Starting program: /usr/lib/R/bin/exec/R
...
> dyn.load("sd.so")
\end{lstlisting}

\section{本书中的其它 R 语言示例}

See these examples (some nonparallel):

in Sections \ref{ompjacobi}, \ref{onedimdft} (nonparallel)
and \ref{smoothing} (nonparallel).

\begin{itemize}

\item Parallel Jacobi iteration of linear equations, Section \ref{ompjacobi}.

\item Matrix computation of 1-D FFT, Section \ref{onedimdft} (can
paralleliza using parallel matrix multiplication).

\item Parallel computation of 2-D FFT, Section \ref{rfft}.

\item Image smoothing, Section \ref{smoothing}.

\end{itemize}



